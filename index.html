<!DOCTYPE html>

<html lang="en">

  <head>
  <style>
  h2.ex1 {
    padding: 35px;
}
  h4.ex1 {
    padding: 10px;
	}
</style>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>:angry: - ECE3400</title>
	<link rel="icon" href="img/angry-face.png">

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

    <!-- Plugin CSS -->
    <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet" type="text/css">

    <!-- Custom styles for this template -->
    <link href="css/freelancer.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg bg-secondary fixed-top text-uppercase" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">:angry:</a>
        <button class="navbar-toggler navbar-toggler-right text-uppercase bg-primary text-white rounded" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item mx-0 mx-lg-1">
              <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#about">About</a>
            </li>
            <li class="nav-item mx-0 mx-lg-1">
              <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#team">The Team</a>
            </li>
            <li class="nav-item mx-0 mx-lg-1">
              <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#labs">Labs</a>
            </li>
            <li class="nav-item mx-0 mx-lg-1">
              <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#milestones">Milestones</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead text-white text-center">
      <div class="container">
        <img class="img-fluid mb-4 d-block mx-auto" src="img/angry-face.png" alt="The Facebook Angry React">
        <h1 class="text-uppercase mb-0">:angry:</h1>
        <hr class="star-light">
        <h2 class="font-weight-light mb-0">
          1ST PLACE || ECE 3400 Fall 2018
        </h2>
      </div>
    </header>

    <!-- About Section -->
    <section class="bg-primary text-white mb-0" id="about">
      <div class="container">
        <h2 class="text-center text-uppercase text-white">About</h2>
        <hr class="star-light mb-5">
        <div class="row">
          <div class="col-lg-5 ml-auto" id="video">
            <a href="https://youtu.be/NN6Hub6EWgc">
              <img class="img-fluid mb-3 d-block mx-auto" src="img/FinalPhotos/robo_portrait.jpg" alt="The Robot">
              <!-- I apologize for the profanity in this video - I hadn't realized this practice run would end up here! -Kevin -->
            </a>
            <p id="unhide">Click or hover over parts of our website to see more.</p>
            <p id="list">Click to see our robot win the final round of the competition! :)</p>
          </div>
          <div class="col-lg-7 mr-auto">
            <p class="lead">Team 28, also known as :ANGRY: or the Angry Reacts, is the highest numbered - and highest quality - team in the ECE 3400 maze exploration robot competition.</p>
            <p>Our robot is fully autonomous and capable of line following, wall-following, tone detection, and IR detection and avoidance of other robots. In addition, it communicates its current location and surroundings <a class="portfolio-item" href="#portfolio-modal-3">using an RF transceiver</a> to an external GUI base station as we map it. Our robot explores its surroundings with the <a class="portfolio-item" href="#portfolio-modal-7">Depth First Search graph algorithm</a>, and while not included in the competition robot, we <a class="portfolio-item" href="#portfolio-modal-4">integrated a camera and FPGA</a> to detect visual treasures. Our robots were built with an Arduino Uno, a DE0-nano FPGA, custom 3D-printed components, and various sensors and servos. To hear more about the competition, please take a look at the <a href="https://cei-lab.github.io/ece3400-2018/">ECE 3400 course page</a>.</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Team Section -->
    <section class="bg-dark text-white mb-0" id="team">
      <div class="container">
        <h2 class="text-center text-uppercase text-white ex1">
          The Team
        </h2>
        <div class="row">
          <div class="col-sm-3 text-center">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="img/tony.png" alt="">
              <h4 class="ex1">Anthony Viego</h4>
              <h5 class="font-weight-light">Junior, ECE</h5>
            </div>
          </div>
          <div class="col-sm-3 text-center">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="img/Glenna.png" alt="">
              <h4 class="ex1">Glenna Zhang</h4>
              <h5 class="font-weight-light">Junior, ECE</h5>
            </div>
          </div>
          <div class="col-sm-3 text-center">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="img/Liliet.jpg" alt="">
              <h4 class="ex1">Liliet Sosa</h4>
              <h5 class="font-weight-light">Junior, ECE</h5>
            </div>
          </div>
          <div class="col-sm-3 text-center">
            <div class="team-member" id="kevin">
              <a href="https://kevinzying.github.io/resume/"><img class="mx-auto rounded-circle" src="img/Kevin.jpg" alt="A Picture of Kevin Ying at the One World Trade Center"></a>
              <h4 class="ex1">Kevin Ying</h4>
              <h5 class="font-weight-light">Junior, ECE</h5>
              <p class="mb-1 small" id="moreinfo">My focus so far has been on IR detection, radio communication, and taking charge for treasure detection (camera I2C communication and FPGA image processor). </p>
              <p class="mb-1 small" id="moreinfo">To check out more of my work, take a look at my online resume <a href="https://kevinzying.github.io/resume/">here</a>. </p>
              <p class="mb-0 small" id="moreinfo">I really just wanted to show off that I found this cool CSS transition on StackExchange. </p>
              <!-- Aforementioned stackexchange link: https://stackoverflow.com/a/30531678 -->
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Lab Grid Section -->
    <section class="portfolio" id="labs">
      <div class="container">
        <h2 class="text-center text-uppercase text-secondary mb-0">Labs</h2>
        <hr class="star-dark mb-5">
        <div class="row">
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-1">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="Lab1/img/lab1.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Microcontroller</p>
            <p class="mb-1 small" id="moreinfo">In which we discover the Arduino Uno and start to get a moving robot.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-2">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/IR_circuit.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Analog Circuitry and FFTs</p>
            <p class="mb-1 small" id="moreinfo">In which we create detect tones and high frequency IR pulses.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-3">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/Lab 3 Photos/GUI.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">System Integration and Radio Communication</p>
            <p class="mb-1 small" id="moreinfo">In which we talk to a GUI, and rediscover all of our sensors.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-4">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/cake.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">FPGA and Shape Detection</p>
            <p class="mb-1 small" id="moreinfo">In which we add a camera and learn the joys of visual processing.</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Milestone Grid Section -->
    <section class="portfolio bg-red text-white" id="milestones">
      <div class="container">
        <h2 class="text-center text-uppercase mb-0">Milestones</h2>
        <hr class="star-light mb-5">
        <div class="row">
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-5">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/robot2.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Line Tracking</p>
            <p class="mb-1 small" id="moreinfo">In which our robot learns to perform a figure eight.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-6">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/Milestone 2 Photos/wallsensor.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Wall Detection</p>
            <p class="mb-1 small" id="moreinfo">In which our robot learns to stop hitting itself.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-7">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/submarine.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Maze Exploration</p>
            <p class="mb-1 small" id="moreinfo">In which our robot learns to search for what it wants.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-8">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/cabin.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Treasure Detection</p>
            <p class="mb-1 small" id="moreinfo">In which our robot learns to tell triangles from squares.</p>
          </div>
        </div>
      </div>
    </section>

	  <footer class="bg-info footer text-center">
      <div class="container">
        <div class="row">
          <div class="col-md-6 mb-0 mb-lg-0">
		    <a class="btn btn-primary btn-lg btn-block" href="https://github.com/ECE3400Team28/website/">View Code on GitHub</a>
		    <a class="portfolio-item btn btn-secondary btn-lg btn-block" href="#contract-modal">View Team Contract</a>
        <a class="portfolio-item btn btn-primary btn-lg btn-block" href="#ethics-modal">View Ethics Assignment</a>
		  </div>
        </div>
      </div>
    </footer>

    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
    <div class="scroll-to-top d-lg-none position-fixed ">
      <a class="js-scroll-trigger d-block text-center text-white rounded" href="#page-top">
        <i class="fa fa-chevron-up"></i>
      </a>
    </div>

    <!-- Portfolio Modals -->

    <!-- Portfolio Modal 1 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-1">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Lab 1: Microcontroller</h2>
              <hr class="star-dark mb-5">

              <h3 class="mb-4">Description</h3>
              <p class="mb-2 text-left">This lab introduces the Arduino Uno and Arduino IDE, and ends with the assembly of a basic robot performing autonomous driving. The lab takes approximately three hours, and requires the following <strong>materials</strong>:</p>
              <ul class="mb-5 text-left">
                <li class="li-center">1x Arduino Uno</li>
                <li class="li-center">1x USB A/B cable</li>
                <li class="li-center">2x Continuous Parallax Servos</li>
                <li class="li-center">1x Solderless breadboard</li>
                <li class="li-center">1x 1k-100k Potentiometer</li>
                <li class="li-center">1x LED, any color</li>
                <li class="li-center">Several 1k and 330 Ohm resistors</li>
              </ul>
              <hr>
              <div class="media">
				        <div class="media-left">
        				  <div class="embed-responsive video">
                      <iframe width="560" height="315" src="https://www.youtube.com/embed/ZAQedv_tLnw?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        			 	  </div>
			          </div>
      			    <div class="media-body">
      			      <h4 class="media-heading">Blinking an internal LED</h4>
      			      <span>After installing the <a href="https://www.arduino.cc/en/Main/Software">Arduino IDE</a> and any necessary drivers, we found the Blink.ino example provided under <mark>File</mark> >> <mark>Examples</mark> >> <mark>1.Basics</mark> >> <mark>Blink</mark>, which toggles the board’s built-in LED once per second.</span>
      			    </div>
			        </div>
      			  <hr>
      			  <div class="media mb-2">
      			    <div class="media-body">
      			      <h4 class="media-heading">Blinking an external LED</h4>
                  <span>In order to test the other digital pins provided on the Uno, we created a global variable named <code>LED_PIN</code>, and modified the existing code to use this pin. We then changed the number assigned to the variable to test each digital pin. The following example tests digital pin 7.</span>
      			    </div>
      			    <div class="media-right">
      		       	<div class="embed-responsive video">
    				        <iframe width="560" height="315" src="https://www.youtube.com/embed/-SuszBuhR4I?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    		 	  	    </div>
      			    </div>
      			  </div>
              <pre class="text-left">
                <code>
const short LED_PIN = 7;

// the setup function runs once when you press reset or power the board
void setup() {
  // initialize digital pin LED_PIN as an output.
  pinMode(LED_PIN, OUTPUT);
}

// the loop function runs over and over again forever
void loop() {
  digitalWrite(LED_PIN, HIGH);       // turn the LED on (HIGH is the voltage level)
  delay(1000);                       // wait for a second
  digitalWrite(LED_PIN, LOW);        // turn the LED off by making the voltage LOW
  delay(1000);                       // wait for a second
}
                </code>
              </pre>
      			  <hr>
      			  <div class="media mb-3">
      			  	<div class="media-left">
      		       	<div class="embed-responsive video">
      				      <iframe width="560" height="315" src="https://www.youtube.com/embed/9hQ8VjL0_DI?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      		 	  	  </div>
      			    </div>
      			    <div class="media-body">
                  <h4 class="media-heading">Reading the value of a potentiometer via the serial port</h4>
                  <span>For this portion of the lab we used a potentiometer to change the voltages on the analog input pin of the arduino, and then programmed the arduino to output these voltages to the serial monitor.</span>
      			    </div>
      			  </div>
              <p class="mb-2 text-left">To do this we started serial communication and then read the analog input using <code>analogRead(INPUT_PIN)</code>. It should be noted that this does NOT return the actual voltage.</p>
              <pre class="mb-0 text-left">
                <code>
int INPUT_PIN = A0;

// the setup function runs once when you press reset or power the board
void setup() {
  Serial.begin(9600);
}

// the loop function runs over and over again forever
void loop() {
  delay(100);

  // Analog input read
  int input = analogRead(INPUT_PIN);
  Serial.println(input);
}
                </code>
              </pre>
              <p class="text-left mb-5">To set up the circuit we wired the output of the potentiometer to the Arudino’s A0(analog input) pin. We then supplied VCC to the wiper pin of the potentiometer and attached a 330 ohm resistor the the output of the potentiometer and GND to form a voltage divider. </p>
      			  <hr>
      			  <div class="media mb-3">
      			    <div class="media-body">
      			      <h4 class="media-heading">Mapping values to the LED</h4>
      			      <span>To simulate and display an analog output, we connected an LED in series with a 330 ohm resistor to a digital pin with PWM capability (pin ~11). Then, we used our code for the potentiometer to pass its value as an input to the <code>analogWrite</code> function, along with the pin connected to the LED. </span>
      			    </div>
      			    <div class="media-right">
      		       	<div class="embed-responsive video">
      				      <iframe width="560" height="315" src="https://www.youtube.com/embed/3bNpFlZQYro?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      		 	  	  </div>
      			    </div>
              </div>
              <p class="mb-2 text-left">We had to scale the potentiometer value because <code>analogWrite</code> only writes values from 0 to 255, whereas the 10-bit analog input results in a value from 0 to 1023. Different values for the potentiometer changed the brightness of the LED. The code is shown below.</p>
              <pre class="mb-0 text-left">
                <code>
const short LED_PIN = 11;
int INPUT_PIN = A0;

// the setup function runs once when you press reset or power the board
void setup() {
  // initialize digital pin LED_PIN as an output.
  pinMode(LED_PIN, OUTPUT);
  Serial.begin(9600);
}

// the loop function runs over and over again forever
void loop() {
  delay(100);

  // Analog input read
  int input = analogRead(INPUT_PIN);
  Serial.println(input);

  // PWM
  analogWrite(LED_PIN, input/4);
}
                </code>
              </pre>
      			  <hr>
      			  <div class="media mb-3">
      			  	<div class="media-left">
      		       	<div class="embed-responsive video">
      				      <iframe width="560" height="315" src="https://www.youtube.com/embed/PLP5Jh7I-W8?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      		 	  	  </div>
      			    </div>
      			    <div class="media-body">
      			      <h4 class="media-heading">Mapping values to the servo</h4>
                  <span>We later used the value from the potentiometer to control a parallax servo. Similarly to the LED, we connected the servo to a PWM pin on the arduino (pin ~10). However, instead of using <code>analogWrite</code>, we imported the <code>Servo.h</code> library which takes care of setting the pin as an output and generating a proper waveform when we use its <code>attach()</code> and <code>write()</code> functions.</span>
      			    </div>
              </div>
              <p class="mb-2 text-left">Again, the values had to be scaled, this time to the range of 0 to 180 that the servo library takes in for its write function. We also added some initial code which demonstrates the servo moving at full speed in both directions, and at a stop.</p>
              <pre class="mb-0 text-left">
                <code>
const short SERVO_PIN = 10;
int INPUT_PIN = A0;
Servo angrySpin;

// the setup function runs once when you press reset or power the board
void setup() {
  Serial.begin(9600);
  angrySpin.attach(SERVO_PIN);
  angrySpin.write(0);
  delay(1000);
  angrySpin.write(90);
  delay(1000);
  angrySpin.write(180);
  delay(1000);
}

// the loop function runs over and over again forever
void loop() {
  delay(100);

  // Analog input read
  int input = analogRead(INPUT_PIN);
  Serial.println(input);

  // PWM
  angrySpin.write(input/6);
}
                </code>
              </pre>
      			  <hr>
      			  <div class="media mb-3">
      			    <div class="media-body">
      			      <h4 class="media-heading">Assembling our robot</h4>
      			      <span>To assemble our robot we used:</span>
                  <ul>
                    <li>A chassis</li>
                    <li>Two wheels</li>
                    <li>Several screws</li>
                    <li>A 9V battery</li>
                    <li>Ball bearing</li>
                    <li>Two servos</li>
                    <li>Arduino Uno</li>
                  </ul>
      			    </div>
      			    <div class="media-right">
      				    <img class="media-object img-fluid" src="img/Robot.png" alt="Assembling our robot">
      			    </div>
      			  </div>
              <p class="mb-5 text-left">To begin, we attached the servos to the mounts, and then attached them to the base of the robot. We then attached the wheels, which took some time because not all wheels fit nicely into the servos. We had to get a bit creative when attaching the ball bearing because there was only one left and it was too short for our robot. We ended up attaching a piece to the base of the robot to elongate the part where the ball bearing attached. This temporarily solved the issue. Lastly, we mounted the arduino and the battery with velcro.</p>
      			  <hr>
      			  <div class="media mb-3">
      			  	<div class="media-left">
      		       	<div class="embed-responsive video">
      				      <iframe width="560" height="315" src="https://www.youtube.com/embed/JGP3CFYPiuo?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      		 	  	  </div>
      			    </div>
      			    <div class="media-body">
      			      <h4 class="media-heading">Driving our robot autonomously</h4>
      			      <span>After assembling the robot, we wrote code for it to move autonomously. Initially, it was meant to move in a square, but the code did not work as expected and we did not have enough time to debug. The robot ended up moving in what appeared to be a pentagon.</span>
      			    </div>
      			  </div>
              <pre class="mb-0 text-left">
                <code>
void setup() {
  // we set up two of our pins as outputs and attach our servos
    int PWM1 = 3;
    int PWM2 = 5;
    pinMode(PWM1, OUTPUT);
    pinMode(PWM2, OUTPUT);
    Motor1.attach(PWM1);
    Motor2.attach(PWM2);
}

void loop() {
  // we write a loop to control the speed and direction that the motors spin
    Motor1.write(180);
    Motor2.write(0);
    delay(1200);
    Motor1.write(90);
    Motor2.write(90);
    delay(600);
    turn();
}

void turn(){
  // this function turns the robot
    Motor1.write(180);
    Motor2.write(180);
    delay(350);
    Motor1.write(90);
    Motor2.write(90);
    delay(250);
    return;
}
                </code>
              </pre>
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Lab1">View Lab 1 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Lab 1</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 2 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-2">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Lab 2: Analog Circuitry and FFTs</h2>
              <hr class="star-dark mb-5">
              <h3 class="text-secondary mb-1">Acoustic Team</h3>
              <h4 class="text-secondary mb-3">Liliet Sosa and Glenna Zhang</h4>
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-5 text-left">
                Create a microphone circuit that will detect a 660 Hz whistle blow signifying the beginning of our maze mapping.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Prelab</h4>
              <p class="mb-3 text-left">
                We need at least a 660Hz x 2 = 1320Hz sampling rate based on the Nyquist sampling rate to avoid aliasing.<br />
                Using the sketch found <a href="https://playground.arduino.cc/Main/ShowInfo">here</a>, these are the results of our speed test:
              </p>
              <img class="img-fluid mb-5" src="img/Lab 2 Photos/ShowInfoResult.png" alt="Serial Monitor result of ShowInfo"></img>
              <p class="mb-3 text-left">
                Based on this, <code>analogRead()</code> is sufficient (don’t need to use ADC) because its maximum rate is 111.987&#181s / sample = 1/111.987e-6 samples/s = 8930Hz, which is more than enough to detect the 660 Hz frequency.<br />
                We used the following materials:
              </p>
              <ul class="mb-5 text-left">
                <li><a href="https://www.mouser.com/datasheet/2/670/cma-4544pf-w-1309465.pdf">Electret microphone</a></li>
                <li>One 6.8&#181F polarized capacitor</li>
                <li>Resistors: 330&#8486, 3.8k&#8486, 380k&#8486, 10k&#8486 x2</li>
                <li>LM358 Op-amp</li>
                <li>Wires</li>
              </ul>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Unit Tests</h4>
              <p class="mb-3 text-left">
                <i>Unit Test 1: Set up a signal generator to deliver a signal that matches what you expect to see from your sensor. This signal must be between 0-5V to not damage the Arduino. Test that the frequency output from your signal generator matches what you see on the serial monitor.
                </i><br />
                <br />
                The following is an image of our signal generator (set to 660Hz, 2V<sub>pp</sub>, 1V offset)
              </p>
              <img class="img-fluid mb-3" src="img/Lab 2 Photos/SigGen.jpg" alt="photo of signal generator"></img>
              <p class="mb-3 text-left">
                We then coded a quick FFT analysis and output the results to the serial monitor:
              </p>
              <img class="img-fluid mb-3" src="img/Lab 2 Photos/SerialOutput.png" alt="photo of signal generator"></img>
              <p class="mb-0 text-left">
                For each line in the image above, the first number is the output of the FFT, and the second number is the index of the bin for readability. As you can see, we have a peak at 0 and at 19. Our bin size should be around 8930Hz / 256samples  = 34.883Hz/bin
                So, our results make sense but are off by seemingly 1 bin.<br />
                <br />
                Our FFT code:
              </p>
              <pre class="mb-0 text-left"><code>
#define LOG_OUT 1 // use the log output function
#define FFT_N 256 // set to 256 point fft

#include &ltFFT.h&gt // include the library

void setup() {
  Serial.begin(9600); // use the serial port
}

void loop() {
  cli();  // UDRE interrupt slows this way down on arduino1.0
  for (int i = 0 ; i < 512 ; i += 2) { // save 256 samples
    fft_input[i] = analogRead(A4); // put real data into even bins
    fft_input[i+1] = 0; // set odd bins to 0
  }
  fft_window(); // window the data for better frequency response
  fft_reorder(); // reorder the data before doing the fft
  fft_run(); // process the data in the fft
  fft_mag_log(); // take the output of the fft
  sei();
  Serial.println("start");
  String out = "";
  for (byte i = 0 ; i < FFT_N/2 ; i++) {
    Serial.println(out + fft_log_out[i] + " " + i); //send out data
  }
  while(1) {} // we inserted this so that it only prints one result
}
              </code></pre>
              <p class="mb-3 text-left">
                <i>Unit Test 2: Use the app you downloaded during the pre-lab to generate a 660Hz tone. Measure the output from the microphone with the oscilloscope, and try to get an idea of what you need to do to the signal to be able to detect it securely from the Arduino.
                </i><br />
                <br />
                The following is the microphone circuit thus far, as taken from the lab handout:
              </p>
              <img class="img-fluid mb-2" src="img/Lab 2 Photos/MicrophoneCircuit_OG.png" alt="Original microphone circuit"></img>
              <p class="mb-3 text-left">
                After generating the tone, the microphone could not pick up the signal, but we knew the microphone was working because if we blew into the microphone the output would obviously change, as shown below:
              </p>
              <p class="mb-4"><b>Resting state of microphone</b></p>
              <img class="img-fluid mb-4" src="img/Lab 2 Photos/Microphone_Resting.jpg" alt="Output of microphone resting"></img>
              <p class="mb-4"><b>Microphone being blown on</b></p>
              <img class="img-fluid mb-4" src="img/Lab 2 Photos/Microphone_Blowing.jpg" alt="Output of microphone resting"></img>
              <p class="mb-3 text-left">
                From this result, we decided to add an amplifier and a filter. After struggling to get our own design to work, we decided to use <a href="https://cei-lab.github.io/ECE3400-2017-teamAlpha/lab2.html">Team Alpha's design</a> that incorporates an inverting amplifier, has a voltage divider to center the output voltage around 2.5V, and has a gain of 100:
              </p>
              <img class="img-fluid mb-2" src="img/Lab 2 Photos/MicCircuit.png" alt="Output of microphone resting"></img>
              <p class="mb-3 text-left">
                <i>Unit Test: Check your circuitry before hooking it up to the Arduino.</i><br />
                <br />
                Below is the result of using the amplifier while playing a 660Hz tone, displayed on the oscilloscope:
              </p>
              <img class="img-fluid mb-3" src="img/Lab 2 Photos/Microphone_Signal.jpg" alt="Output of microphone resting"></img>
              <p class="mb-3 text-left">
                Clearly, this signal can be sent into the Arduino as its minimum voltage is 2.12V and its peak-to-peak voltage is 840mV, which is well within the 0-5V range. The frequency is also correct.<br />
                The following is a picture of the physical circuit, where the purple wire is the output signal:
              </p>
              <img class="img-fluid mb-5" src="img/Lab 2 Photos/Circuit.jpg" alt="Output of microphone resting"></img>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Results</h4>
              <p class="mb-3 text-left">
                After connecting the circuit to our robot and changing the code that controls how the robot starts to incorporate waiting for the 660Hz signal, this is a video of our successful result:
              </p>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/aNj-LKDwgoE?rel=0" frameborder="0" allow="autoplay; encrypted-media" class="mb-3" allowfullscreen></iframe>
              <p class="mb-0 text-left">
                The following is our updated line following code to show that our robot waits for a signal before starting:
              </p>
              <pre class="mb-0 text-left"><code>
// In set-up, we wait while the signal has not been heard and the button has not been pressed:
while(!readSignal() && digitalRead(8) != HIGH);

// The following is our readSignal() method that calculates the FFT and returns true or false based on if the signal has been heard. We are reading the analog signal from A4:
boolean readSignal() {
  cli();  // UDRE interrupt slows this way down on arduino1.0
  for (int i = 0 ; i < 512 ; i += 2) { // save 256 samples
    fft_input[i] = analogRead(A4); // put real data into even bins
    fft_input[i+1] = 0; // set odd bins to 0
  }
  fft_window(); // window the data for better frequency response
  fft_reorder(); // reorder the data before doing the fft
  fft_run(); // process the data in the fft
  fft_mag_log(); // take the output of the fft
  sei();
  if (fft_log_out[19] >= 50){ // check that bin 19 contains a significant value
    return true;
  }
  return false;
}
              </code></pre>
              <hr class="mb-5">
              <h3 class="text-secondary mb-1">Optical Team</h3>
              <h4 class="text-secondary mb-3">Anthony Viego and Kevin Ying</h4>
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-5 text-left">
                Create a circuit which is able to detect a 6.08kHz IR signal signifying other robots which have to be avoided in the maze while discarding 18kHz decoy IR signals.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Prelab</h4>
              <p class="mb-3 text-left">
                Unlike the microphone circuit, using AnalogRead will not be fast enough for IR purposes.
                We want to be able to detect 6kHz signals, which would require a sampling frequency of
                approximately 12kHz. However, the maximum with AnalogRead is approximately 8930Hz,
                as discussed above. Instead, we will have to read directly from the ADC pin result
                register to increase our sampling frequency. Since we will have to sample at such a high frequency,
                it will be difficult to do other processing simultaneously - we will have to carefully
                determine when to measure as to not miss other robots while were are making decisions
                about where to drive.
              </p>
              <p class="mb-3 text-left">
                Thankfully, we don't have to worry as much about other sources of IR, which are unlikely to occur at the same frequency. For example, one common source of IR is fluorescent lights, but they emit approximately 30-60kHz signals. Testing in lab confirms that even in the presence of artificial lights and sunlike, the main source of noise for IR is 60Hz coming from the walls.
              </p>

              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Frequency Detection and Amplification Circuit</h4>
              <p class="mb-3 text-left">
                In order to build the amplification circuit, we needed the following materials in addition to our usual Arduino Uno setup:
              </p>
              <ul class="mb-5 text-left">
                <li class="li-center">2x OP598A Phototransistor</li>
                <li class="li-center">2x LM358 Dual OP-Amp</li>
                <li class="li-center">1x 430 Ohm Resistor</li>
                <li class="li-center">1x Solderless breadboard</li>
                <li class="li-center">2x 1k Resistor</li>
                <li class="li-center">1x 10k Resistor</li>
                <li class="li-center">1x 100k Resistor</li>
                <li class="li-center">1x 24k Resistor</li>
                <li class="li-center">1x 10nF Capacitor</li>
              </ul>
              <p class="mb-3 text-left">
                Overall our circuit for detecting other robots can be broken down into 3 stages.
                A picture of our completed IR circuit is below. You can see that one the top right
                of the image, there exist two phototransistors in series. That then feeds into the
                op-amp ICs, and the output commes out of the light blue wire which goes off-screen
                to the left.
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/IR_circuit.jpg" alt="Completed IR Capture and Amplification Circuit"></img>
                <figcaption>Figure: Completed IR Capture and Amplification Circuit</figcaption>
              </figure>

              <h5 class="text-secondary text-left mb-3">Stage 1: IR Capture</h4>
              <p class="mb-3 text-left">
                The first stage of our circuit is similar to the suggested circuit in the <a href="https://cei-lab.github.io/ece3400-2018/lab2.html">lab 2 handout</a>. However, rather we chose to use two resistors in parallel and two phototransistors in series to increase the amplitude of the produced signal. Figure 1 shows this stage of the circuit. As the output of our circuit ranged from 20mVpp to 100mVpp at very close distances, we decided to amplify the output of this stage to increase the distance we can detect other robots from.
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/IR_capture_circuit.png" alt="IR Capture Circuit"></img>
                <figcaption>Figure: IR Capture Circuit</figcaption>
              </figure>
              <p class="mb-3 text-left">
                Once we implemented the output capture circuit, we probed the output of
                the first stage with an oscilloscope when holding a treasure close to
                the phototransistors. The treasures were tunable in frequency, so we
                were able to output both a 6.2kHz signal and a 18kHz signal, which
                approximate the frequencies emitted by the other robots and the decoy
                robots in the maze respectively. While there is a visible signal, its
                peak-to-peak voltage is very small, meaning that the Arduino would not
                easily be able to tell when the signal was present. In addition, in
                order to even get this clear of a signal, we needed to hold the IR
                source very close to the phototransistors, which would not be a viable
                solution for detecting robots from several inches away. We needed to
                add analog amplification.
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/scope_no_amp.png" alt="Oscilloscope Output of IR Signal Before Amplification"></img>
                <figcaption>Figure: Output of IR Signal Before Amplification</figcaption>
              </figure>

              <h5 class="text-secondary text-left mb-3">Stage 2: Amplification</h4>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/ir_amp_circuit.png" alt="Amplification Circuit Diagram"></img>
                <figcaption>Figure: Amplification Circuit Diagram</figcaption>
              </figure>
              <p class="mb-3 text-left">
                This stage is responsible for amplifying the output of our IR capture circuit.
                The circuit itself can be seen in figure 2 below. It should be noted that the
                output of the IR capture circuit has a DC bias of approximately 2.5V. Since we
                plan to have 20x gain, this would mean our signal would always hit the max voltage
                of 5V and would appear to be a constant high signal. Thus, to remove this bias
                we implemented a high pass RC filter using a 10k resistor and 10nF capacitor
                which has a cutoff frequency of roughly 1592Hz.
              </p>
              <p class="mb-3 text-left">
                We then fed the output of the low pass filter into a non-inverting amplifier.
                We used a 20k resistor and 1k resistor set up in a negative feedback loop to
                achieve a gain of roughly 20x. We found that this amplification level gave us
                the clearest results on our output.
              </p>

              <h5 class="text-secondary text-left mb-3">Stage 3: Comparator</h4>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/ir_comp_circuit.png" alt="Comparator Circuit Diagram"></img>
                <figcaption>Figure: Comparator Circuit Diagram</figcaption>
              </figure>
              <p class="mb-3 text-left">
                In order to further amplify our output, we fed the output of the amplification
                circuit to a comparator circuit, which we hoped would further increase the
                gain of our circuit and improve the signal quality. We first sent the input
                through a unity gain voltage buffer which ensured that as we experimented
                with different types of amplifiers and comparator circuits, the gain of each
                would act independently. With our final design, we no longer need the unity
                gain amplifier and may remove it to compact our circuit.
              </p>
              <p class="mb-3 text-left">
                The second half uses a comparator with a voltage generated with a voltage divider.
                We first determined a voltage that we wanted to compare against by stepping
                through voltage levels on a DC power supply until the desired result was achieved.
                We found that we were able to get the clearest signal from the greatest range of
                distances for the IR treasure for a comparator reference voltage of
                approximately 23mV. The values of the resistors in the voltage divider were then
                determined to approximate that voltage, since 5V * (430/100000) = 21.5mV.
              </p>
              <p class="mb-3 text-left">
                With the new amplifier and comparator, we were able to achieve the following output even when the IR emitter treasure was held 3-4 inches away from the phototransistors.
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/ir_with_amp.png" alt="Oscilloscope Output of IR Signal With Amplification"></img>
                <figcaption>Figure: Output of IR Signal With Amplification</figcaption>
              </figure>

              <h4 class="text-secondary text-left mb-3">FFT</h4>
              <p class="mb-3 text-left">
                Once we had a high signal-to-noise ratio and a clear signal, we now wanted to
                be able to read data into the Arduino and process the inputs to determine the
                frequencies coming in. To do this we connected the output of our comparator
                circuit to pin A0 of the arduino. We then used adc0 to read in the output of
                the comparator circuit from the ADCL and ADCH registers and converted these
                values into a single 16 bit integer. These values were then stored in the
                <code>fft_input</code> array which was then used by the fft library to calculate the
                magnitude of each bin. The graph below shows the result of this operation
                for a 6kHz and 18kHz signal.
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/ir_6k_18k_comparison.png" alt="Overlaid FFT Output for Amplified IR Sensor Signal"></img>
                <figcaption>Figure: Overlaid FFT Output for Amplified IR Sensor Signal</figcaption>
              </figure>
              <p class="mb-3 text-left">
                As can be seen, the peak magnitude of the 6kHz signal is around bin 40. This
                result is relatively accurate as our base clock for the arduino is 16MHz with
                an ADC clock prescaler of 32. As each ADC read takes 13 clock cycles this
                results in a sampling frequency of 38461.53. As we take 256 samples this
                means that each bin represents 150.24 Hz. If we divide 6.08kHz by 150.24Hz
                we find that the peak of our 6kHz single should be in bin 40 which is what
                we found.
              </p>
              <p class="mb-3 text-left">
                We then use this information to determine when a 6kHz signal is present by
                checking to see if the magnitude of bins 38 - 44 are above 150. We decided
                to use 150 as our threshold since general noise did not seem to create peaks
                above 75 and the 18kHz signal also does not generate peaks in that range
                above 100. By checking a range of bins we are able to determine if an
                approximate 6kHz signal is present while having some tolerance incase the
                frequency is not exact. The code, which is inside of a loop, is as follows:
              </p>
              <pre class="mb-0 text-left"><code>
cli();  // UDRE interrupt slows this way down on arduino1.0 so we disable interrupts

// save 256 samples
for (int i = 0 ; i < 512 ; i += 2) {
  while(!(ADCSRA & 0x10)); // wait for adc to be ready
  ADCSRA = 0xf5; // restart adc
  byte m = ADCL; // fetch adc data
  byte j = ADCH;
  int k = (j << 8) | m; // form into an int
  k -= 0x0200; // form into a signed int
  k <<= 6; // form into a 16b signed int
  fft_input[i] = k; // put real data into even bins
  fft_input[i+1] = 0; // set odd bins to 0
}

// process FFT data
fft_window(); // window the data for better frequency response
fft_reorder(); // reorder the data before doing the fft
fft_run(); // process the data in the fft
fft_mag_log(); // take the output of the fft

sei(); // re-enable interrupts
Serial.println("start");
for (byte i = 0 ; i < FFT_N/2 ; i++) {
  Serial.println(fft_log_out[i]); // send out the data for off-chip processing
}
delay(1000);
for (int j = 38; j < 44; ++j) {
  if (fft_log_out[j] >= 150){
    digitalWrite(LED_BUILTIN, HIGH);   // turn the LED on (HIGH is the voltage level)
    delay(1000);                       // wait for a second
    digitalWrite(LED_BUILTIN, LOW);    // turn the LED off by making the voltage LOW
    delay(1000);                       // in reality, delay does not block other cmds so LEDs flash
  }
}
              </code></pre>
              <p class="mb-3 text-left">
                The results can be shown below.
              </p>
              <iframe class="mb-5" width="560" height="315" src="https://www.youtube.com/embed/GuVrt4HzFvU?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

              <hr class="mb-5">
              <h3 class="text-secondary mb-1">The Final Integration</h3>
              <p class="mb-3 text-left">
                We then combined the results of the IR team and the acoustic team into one larger project!
              </p>
              <iframe class="mb-5" width="560" height="315" src="https://www.youtube.com/embed/RVqd22eCrKk?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Lab2">View Lab 2 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 3 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-3">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Lab 3: System Integration and Radio Communication</h2>
              <hr class="star-dark mb-5">
              <h3 class="text-secondary mb-1">Radio Team</h3>
              <h4 class="text-secondary mb-3">Kevin Ying and Glenna Zhang</h4>
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-5 text-left">
                In this lab we established a communication protocol between one Arduino, which will be on our robot, and another, which will remain at a base station and need to interface with a GUI to display the progress of our robot. This interface will also come in handy for debugging because we will be able to determine the progress of our robot and what it has discovered.<br />
                <br />
                In order to communicate wirelessly, we used a pair of Nordic nRF24L01+ transceivers, as well as the relevant libraries. (can be found <a href="https://cei-lab.github.io/ece3400-2018/lab3.html">here</a>)
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Data Scheme & Protocol</h4>
              <p class="mb-3 text-left">
                We use the following two variables to store the current location of our robot:<br />
                <code>uint8_t x = 0; // stores the x index/coordinate</code><br />
                <code>uint8_t y = 0; // stores the y index/coordinate</code><br />
                We use the following 2-D array to store what we discover about each tile of the maze:<br />
                <code>uint8_t maze[9][9] = {...};</code>
                <br />
                We have also defined a set of macros that help us easily construct the correct bit sequence that describes what our robot sees, based on our protocol:
              </p>
              <pre class="mb-0 text-left">
                <code>
// walls
#define bm_wall       15 << 0
#define bm_wall_east  1 << 1
#define bm_wall_north 1 << 3
#define bm_wall_west  1 << 0
#define bm_wall_south 1 << 2

// treasure
#define treasure_shift 4
#define bm_treasure_none     0 << 4
#define bm_treasure_b_sq     1 << 4
#define bm_treasure_r_sq     2 << 4
#define bm_treasure_b_ci     3 << 4
#define bm_treasure_r_ci     4 << 4
#define bm_treasure_b_tr     5 << 4
#define bm_treasure_r_tr     6 << 4

// whether square explored
#define bm_explored     1 << 7
#define bm_not_explored 0 << 7
#define explored_shift  7

// presence of other robot
#define bm_robot    1 << 1
#define bm_no_robot 0 << 1
#define robot_shift 1
                </code>
              </pre>
              <p class="mb-3 text-left">
                We learned that the maximum package the Nordic Radio module can send is 32 bytes, but we realized we only need to send 2 bytes of data to fully describe each tile, which means we were able to use a single transmission/packet per tile/cell. The following image describes the structure of our protocol:
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 3 Photos/Protocol.png" alt="Protocol of our communication method"></img>
                <figcaption>Figure 1: Communication protocol</figcaption>
              </figure>
              <p class="mb-5 text-left">
                We believe that this protocol provides sufficient information about the state of the board. It includes the x and y coordinates of the current tile, which has just been explored. The explored bit is mainly for the robot itself to know if it has explored a tile. The treasure bits represent one of seven options: no treasure, blue square, red square, blue circle, red circle, blue triangle, and red triangle. Lastly, the for the bits representing each wall (north, south, east, west), a 1 represents that the wall is present in this tile and a 0 means it isn’t there.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Sending maze information wirelessly between Arduinos</h4>
              <p class="mb-3 text-left">
                We wrote a program to make a virtual robot explore a 9x9 maze with preset walls and treasures in order to test our radio communication and our protocol. We hard-coded the robot’s sequence of exploration for ease- it just snakes around the maze, as there is only one path anyway. We set the role of the virtual robot to “role_ping_out”, a.k.a. the transmitter, and we set the other Arduino to “role_pong_back”, or the receiver.<br />
                <br />
                The following is our setup code, which is very similar to the setup found in the GettingStarted sketch in the RF24 Arduino library:
              </p>
              <pre class="mb-0 text-left">
                <code>
void setup(void) {
  // Print preamble
  Serial.begin(9600);
  printf_begin();
  printf("\n\rRF24/examples/GettingStarted/\n\r");
  printf("ROLE: %s\n\r",role_friendly_name[role]);
  printf("*** PRESS 'T' to begin transmitting to the other node\n\r");

  // Setup and configure rf radio
  radio.begin();
  radio.setRetries(15,15);
  radio.setAutoAck(true);
  radio.setChannel(0x50);
  // set the power
  radio.setPALevel(RF24_PA_HIGH);
  radio.setDataRate(RF24_250KBPS);

  // optionally, reduce the payload size.  seems to
  // improve reliability
  radio.setPayloadSize(2); // we only need 2 bytes

  // Open pipes to other nodes for communication
  // This simple sketch opens two pipes for these two nodes to communicate
  // back and forth.
  // Open 'our' pipe for writing
  // Open the 'other' pipe for reading, in position #1 (we can have up to 5 pipes open for reading)
  radio.openWritingPipe(pipes[0]);
  radio.openReadingPipe(1,pipes[1]);

  // Start listening
  radio.startListening();

  // Dump the configuration of the rf unit for debugging
  radio.printDetails();
}
                </code>
              </pre>
              <p class="mb-0 text-left">
                Then, in the loop, we first develop the message that we are trying to send using the current cell information and the current location. We also print the message in binary for debugging:
              </p>
              <pre class="mb-0 text-left">
                <code>
uint8_t cell = maze[x][y]; // information about the current location, which our robot will use its sensors to determine
uint16_t coordinate = x << 4 | y; // generate the left byte of information (coordinates)
uint16_t message = coordinate << 8 | cell; // generate the whole message
Serial.println(message, BIN);
                </code>
              </pre>
              <p class="mb-0 text-left">
                The rest of the communication code is the same as in the GettingStarted sketch, but we changed what happens when the response is received- if a good response is received, we then let the robot move to the next tile, but if no response is received (timeout), then the robot stays put and tries again in the next iteration. This way no information is lost. The code is below:
              </p>
              <pre class="mb-0 text-left"><code>
// Describe the results
if ( timeout )
{
  printf("Failed, response timed out.\n\r");
}
else
{
  // Grab the response, compare, and send to debugging spew
  unsigned long got_time;
  radio.read( &got_time, sizeof(unsigned long) );

  // Spew it
  printf("Got response %lu, round-trip delay: %lu\n\r",got_time,millis()-got_time);

  if (x == 8 && y == 8) {
      x = 0;
      y = 0;
  }
  else if (x%2 == 0){
      if (y == 8) x++;
      else y++;
  }
  else{
      if (y == 0) x++;
      else y--;
  }
}

 // Try again 1s later
delay(1000);
              </code></pre>
              <p class="mb-0 text-left">
                We wrote another program to be the receiver of the messages and to update the GUI. The setup is largely the same as the transmitter program, so we will only show the looping code. It receives a packet and then proceeds to decode it using bitwise operators and our macros, and then sends the information in the correct format to the GUI (location, walls, and treasures, then a new line). Lastly, it resumes listening for a new packet.
              </p>
              <pre class="mb-0 text-left"><code>
// if there is data ready
if ( radio.available() )
{
  // Dump the payloads until we've gotten everything
  uint16_t message;
  unsigned char coord;
  unsigned char walls;
  bool done = false;
  while (!done)
  {
    // Fetch the payload, and see if this was the last one.
    // spew out payloads
    done = radio.read( &message, sizeof(uint16_t) );

    int x = (message & (15 << xcoord_shift)) >> xcoord_shift;
    int y = (message & (15 << ycoord_shift)) >> ycoord_shift;
    bool north = message & bm_wall_north;
    bool south = message & bm_wall_south;
    bool east = message & bm_wall_east;
    bool west = message & bm_wall_west;
    uint8_t treasure = (message & bm_treasure);
    char buff[30];
    sprintf(buff, "%d,%d", x, y);
    Serial.print(buff);

    /*
     * Code for checking the message with each wall/treasure bitmask goes here
     * If a match is found, more is printed to Serial
     */

    // Ends printed message to GUI
    Serial.print("\n");

    // Delay just a little bit to let the other unit
    // make the transition to receiver
    delay(20);

  }

  // First, stop listening so we can talk
  radio.stopListening();

  // Send the final one back.
  radio.write( &coord, sizeof(unsigned long) );
  //printf("Sent response.\n\r");

  // Now, resume listening so we catch the next packets.
  radio.startListening();
}
              </code></pre>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Updating the GUI</h4>
              <p class="mb-3 text-left">
                This is the final result of using our virtual robot to wirelessly update the GUI!
              </p>
              <iframe class="mb-5" width="560" height="315" src="https://www.youtube.com/embed/oeo-r_0Xa80?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <hr class="mb-5">
              <h3 class="text-secondary mb-1">Robot Team</h3>
              <h4 class="text-secondary mb-3">Liliet Sosa and Anthony Viego</h4>
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-3 text-left">
                The goal for this part of the lab was to integrate all of our robot’s sensing components, including recognizing the start signal of 660Hz with a microphone, line following, right-hand wall following, and detecting other robots while ignoring decoys. All of these features have been successfully built in previous labs/milestones, and so now we’ve integrated all of these features to prove that they can work together without interfering with each other.
              </p>
              <p class="mb-3 text-left">
                Integrating all of our separate circuits and systems in order to complete lab 3 turned out to be more of a challenge than initially anticipated. For starters, our IR detection circuit, microphone circuit, sensor inputs, and servo controls all existed either on different breadboards or very far away from the arduino. This created a large mess of wires that made modifying and debugging our circuits difficult as every modification almost always resulted in accidently removing another wire.
              </p>
              <p class="mb-3 text-left">
                Cleaning up, we moved all of our circuitry to two half-breadboards which we mounted on the top level of our robot, with our arduino now being mounted on the lower level. This gave us better access to all of our circuitry as well as it helped to reduce the nest of wires making our robot easier to debug and cleaner looking as well. However, as we re-wired out inputs and outputs from our arduino we ran into the problem that we had more analog inputs than the arduino could handle.
              </p>
              <p class="mb-3 text-left">
                To get around this problem, we used the CD74HC4051 analog multiplexer to switch between our wall sensors, since we only needed to be able to read from those sensors fairly infrequently. As we currently only needed to mux between two inputs we grounded the S1 and S2 selection inputs and connected digital pin 7 of our arduino to S0 of the mux. We then connected the output channel of the mux to analog input pin A5.
              </p>
              <p class="mb-3 text-left">
                A rather unanticipated issue that arose from using the mux was that both of our sensors appeared to be changing based off of the values from the right one. At first we believed that one of the sensors was simply broken since the mux had been functioning originally. However, we discovered that the mux does not sit properly in the breadboard, causing some of the pins to not be fully connected. This resulted in only one of the wall sensors ever being read. To fix this problem we needed to bend several of the pins in order to better fit the breadboard.
              </p>
              <p class="mb-3 text-left">
                Another unanticipated problem we faced was the noise on our 5V rails caused by the wall sensors. Due to this noise our IR and microphone circuits ceased to function correctly. Our initial solution to this problem was to add as many decoupling capacitors as possible but we found that this simply didn’t work well enough. Instead, we decided to use the provided 5-to-3.3 voltage regulator, in order to have a separate power line dedicated to the sensitive IR circuit.
              </p>
              <p class="mb-5 text-left">
                Finally, in order to integrate the maze GUI code with the robot maneuvering, we needed to keep track of the current orientation of the robot (which would get updated whenever we turn left/right), as well as update the internal maze state whenever we reach an intersection, at which point we are able to detect walls. Note that since we only broadcast the maze state one time, before it has been explored, some of the walls do not appear. This will be fixed once add a third wall sensor on the left side of the robot, so that we will be able to detect all of the walls at the same time.
              </p>
              <hr class="mb-5">
              <h3 class="text-secondary mb-1">Final Integration</h3>
              <p class="mb-3 text-left">First, we incorporated radio communication into our overall program, as shown in the video below. As you can see in the video, the robot now pauses for a bit at every intersection. We do this because this is when the robot broadcasts to the base station, and if the broadcast is unsuccessful, it stays put and keeps trying. The coordinates received in the base station are correct at every intersection, based on the starting coordinate of the robot being set to (0, 0). We did not show the actual GUI updating in this video, because it was inconvenient at the time of recording and we've already shown above that it works.</p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/N0hEJD_Hcbo?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <p class="mb-5 text-left">Finally, the following video shows everything incorporated into our robot (line following, wall following, radio communication, GUI, starting on 660Hz, responding to other robots, and ignoring decoys):</p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/Ex6m2g_6p3g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <p class="mb-5 text-left">This last video shows the same robot (running the same code) detecting the IR hat from a farther distance. </p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/sQ3DKX8HY3o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <hr class="mb-5">
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Lab3">View Lab 3 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 4 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-4">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Lab 4: FPGA and Vision</h2>
              <hr class="star-dark mb-5">
              
              <h3 class="text-secondary text-left mb-3">Goal</h3>
              <p class="mb-3 text-left">
                At this point, we want to begin being able to detect visual treasures. Specifically, we need to be able to distinguish normal white walls from specific colored shapes attached to the walls. The treasures can be either blue or red, and can be any of a square, triangle, or diamond. In order to do this, we add a camera (the OV7670). The camera outputs data to an FPGA (the DE0-NANO), which then processes the image in real time in order to determine the presence or absence of any treasures, as well as what color they are. For debugging purposes, we also connected the FPGA to a VGA adaptor and monitor. A datasheet for the OV7670 camera can be found <a href="http://web.mit.edu/6.111/www/f2016/tools/OV7670_2006.pdf">here.</a>  Note that since we deal with a field-programmable gate array, we now need to assign these register conenections in a hardware description language. We chose to write in Verilog compiled in Altera's Quartus Prime Lite softare.  
              </p>
              <img class="img-fluid mb-5" style="max-height:400px" src="img/Lab4Photos/Treasures.png" alt="The six treasure types">
              <p class="mb-3 text-left">
                Above is the visual of the specific types of treasures we wish to detect and differentiate. The graphic was borrowed from the ECE 3400 Fall 2018 course website.
              </p>

              <hr class="mb-5">
              <h3 class="text-secondary mb-1">Choosing our Parameters</h3>
              <p class="mb-3 text-left">
                Before we start, we want to get an idea of what types of images we can store and process in real time, since we need to be able to quickly determine whether or not there is a treasure while still navigating through a maze. The DE0-Nano has 594 Kbits of embedded memory, and each entry in the RAM must have a size of 8 or 9 bits. Assuming 8-bit entries, that leads to a maximum buffer size of 74250 entries.
              </p>
              <p class="mb-3 text-left">
                To start, we wanted to get data quickly from the camera while maintaining the most color accuracy possible, so we receive RGB 565 data from the camera. The format for RGB 565 is below, with the MSByte being sent first by the camera, followed by the LSByte.
              </p>
              <p class="mb-3 text-left">
                Note however that VGA cables require data in a RGB 332 format, which is shown above. This means that we must downsample the given camera data. That is, in order to keep the most important data from the RGB 565 information but fit it into a single bit, we only keep the most significant bits of each of the red, green, and blue channel to feed to the monitor for display.
              </p>
              <p class="mb-3 text-left">
                Since we want to be able to take advantage of large-scale features of the image, and have less use for specific details, we choose to maintain a fairly small image size: we chose to use the smallest supported resolution of QCIF at 176x144 pixels. Many of the other settings were discovered through a series of trial-and-error experiments.
              </p>
              <hr class="mb-5">

              <h3 class="text-secondary mb-1">Team Camera</h3>
              <h4 class="text-secondary mb-3">Kevin Ying and Liliet Sosa</h4>
              <p class="mb-3 text-left">
                The camera team's goal was to communicate to the camera in order to adjust settings to the parameters determined above. The camera team also needed to have some fake-treasure data arising from the FPGA which would test the capability of FPGA to Arduino communication. The details of this communication are detailed in the final integration section of this page.
              </p>
              <p class="mb-3 text-left">
                To communicate with the camera, we use the Inter-Integrated Circuit protocol, also called I2C or the two-wire interface I2C. To do this, we use the dedicated I2C pins on the Arduino Uno, the SCL for clock and SDA for data. There are a number of intricacies to I2C, but Wire Arduino library makes reading and writing fairly easy. Simplified example code to read or write to a specific register is below. The master device, in our case the Uno, sets the clock speed on the SCL line at that rate. The master's message is then sent synchronously over SDA and acknowledged by the slave, and responses are requested by the master as necessary.
              </p>
              <pre class="mb-0 text-left">
                <code>
byte read_register_value(int register_address){
  byte data = 0;
  Wire.beginTransmission(OV7670_I2C_ADDRESS);
  Wire.write(register_address);
  Wire.endTransmission();
  Wire.requestFrom(OV7670_I2C_ADDRESS,1);
  while(Wire.available()<1);
  return Wire.read();
}

int OV7670_write_register(int reg_address, *byte data){{
    int error;
    Wire.beginTransmission(OV7670_I2C_ADDRESS);
    Wire.write(start);
    Wire.write(&Data, 1);
    error = Wire.endTransmission(true);
    if(error != 0){
      return 0;
    }
    return 1;
 }
                </code>
              </pre>
              <p class="mb-3 text-left">
                For the OV7670, the registers we can read and write determine important details of the camera operation, including the clock source, the output format, and gain and white balancing settings. In particular, we took care to write to the following registers:
              </p>
              <ul class="mb-3 text-left">
                <li>0x11, to ensure that we used an external clock generated by the FPGA</li>
                <li>0x12, to reset all registers to default values at the start of operation, as well as determine the QCIF and RGB565 format</li>
                <li>0x14, to freeze the automatic gain at a reasonable level, determined at the initial programming of the camera, in order to avoid unpredictable gain adjustments.</li>
                <li>0x40 and 0x42, in order to first test our FPGA and VGA interface with a color bar test</li>
              </ul>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/lfoqNJq3ZcA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <p class="mb-3 text-left">
                Above is a video of the Uno communicating with the OV7670, reading a list of registers, writing to a subset of them, and then reading the same list of registers in order to confirm that the changes desired were implemented correctly. A complete list of the registers written to can be found in the code. For more information about the register specifics, see the datasheet linked above. 
              </p>

              <hr class="mb-5">
              <h3 class="text-secondary mb-3">FPGA Clock Generation</h3>
              <p class="mb-3 text-left">
                Another critical part of communicating with the camera was the generation of a 24 megahertz clock signal. In order to do this we use a phase locked loop generated by the fpga. We then use the output of the fpga as an input to the camera. More details can be found at the lab website.  
              </p>

              <hr class="mb-5">
              <h3 class="text-secondary mb-1">Team FPGA</h3>
              <h4 class="text-secondary mb-3">Anthony Viego and Glenna Zhang</h4>
              <p class="mb-3 text-left">
                This team's goal was to be able to store data coming in from the camera or some other source, and then display it onto a monitor through VGA.
              </p>
              <p class="mb-3 text-left">
                To do this, we generated 25 and 50 megahertz signals using PLL in order to store and read data from the generated RAM at high rates. Since the camera pixel color data came over two bytes but the camera pins only supported eight bits in parallel, we needed to track whether we were reading in the most significant byte or least significant byte of each pixel and downsample specific bits of that color data accordingly before it was stored in RAM. Note that the camera also outputs synchronization signals, vsync and href, which help the fpga find the start and end of a row or frame,  but we found the signals tended to be noisy so we required a transmission to be fairly clean by tracking the readings over several cycles and requiring a specific pattern.
              </p>
              
              <hr class="mb-5">
              <h3 class="text-secondary mb-1">FPGA and Camera Integration</h3>
              <h4 class="text-secondary mb-3">A Joint Effort</h4>
              <p class="mb-3 text-left">
                The FPGA reads from the stored camera data on each clock cycle and outputs it to VGA alongside other signals determined by the horizontal and vertical positions of the pixels being displayed. We started by manually writing to the FPGA memory in the program, followed by testing communication with the camera and the use of downsampling by displaying a color bar test from the camera, which can be seen below. Note that we used a solid blue color background in order to differentiate the smaller camera/memory output from the static background display pixels.
              </p>
              <img class="img-fluid mb-5" style="max-height:400px" src="img/Lab4Photos/colorbar_s.jpg" alt="Color bar test output on monitor">
              <p class="mb-3 text-left">
                One we were confident in our downsampling and camera stability, we were able to capture life-like images on the screen from the camera, after more experimentation with color and gain settings. An example can be seen below.
              </p>
              <img class="img-fluid mb-5" style="max-height:400px" src="img/Lab4Photos/kevinface3.jpg" alt="Kevin face test output on monitor">
              <p class="mb-3 text-left">
                We generated estimates of whether each pixel was either red, blue, or neither based off of simple thresholding of pixel values, in order to help determine whether an image being read from memory contained a red or blue treasure. For instance, blue treasures tended to appear dark blue or black to our camera, so we required that the red, blue, and green bits all be fairly low to label a pixel blue. On the other hand, red pixels had a lower bound threshold for the red values, as well as other bands for the blue and green values. These bounds were experimentally determined in order to maximize the chance that treasures could be separated from their backgrounds and each other in a variety of different lighting conditions. A simplified version of our code is below.
              </p>
              <pre class="mb-0 text-left"><code>
// PIXEL_IN from RAM, PIXEL_OUT_C goes to VGA
if (PIXEL_IN[1:0] <= 2'b10 && PIXEL_IN[4:2] < 3'b101 && PIXEL_IN[7:5] < 3'b101) begin
  // all fields dark - what tends to be blue
  blue_cnt = blue_cnt + 24'b1;
  PIXEL_OUT_C = 8'b000_000_11;
end
if (PIXEL_IN[7:5] >= 3'b010 && PIXEL_IN[4:2] < 3'b101 && PIXEL_IN[1:0] <= 2'b10) begin
  // anything somewhat red and not too green / bright
  red_cnt = red_cnt + 24'b1;
  PIXEL_OUT_C = 8'b111_000_00;
end
              </code></pre>
              <p class="mb-3 text-left">
                To more easily debug which pixels were believed were to be red, blue, or otherwise, we modified the VGA output to display a solid red pixel for red-thresholded pixels, blue for blue-thresholded, and black for other. We also found that sometimes, white pixels were being thresholded as red, so we generated a third category in order to show pixels the FPGA believed to be white, and therefore should not be counted as red. A video of camera footage post-processing can be seen below.
              </p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/51e3VvY_uiE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <p class="mb-3 text-left">
                We then counted the number of red or blue pixels in a single frame. If the number of red pixels, for example, exceeded another experimentally determined threshold, we would say that the image most likely contains a red treasure. This information was then communicated to the Arduino a pair of output pins which were connected to Arduino GPIO pins. We demonstrate the color detection capabilities with a program which simply writes all of the necessary registers on the camera, and then constantly polls the FPGA-to-Arduino pins in order to determine what the FPGA believes the current state of the presence of treasures is. A video of this code in action can be found below. Since we continued to multiplex analog signals, we were able to utilize a pin-heavy parallel communication protocol.
              </p>
              <img class="img-fluid mb-5" style="max-height:300px" src="img/Lab4Photos/fpga2camera_s.jpg" alt="Schematic of FPGA to Camera Communication">
              <p class="mb-3 text-left">
                Note that some of the code on the GitHub has been updated to include parts of our shape recognition algorithm, part of Milestone 4. Although some of the details have changed, the majority of the colored section code remains the same - the FPGA still counts the number of red or blue pixels in an image in order to different the dominant color and if there is a treasure or not. A video of the demonstration communicating with the Arduino and outputting results to the computer through its Serial port is below.
              </p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/bFCsnk18t3w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <p class="mb-5 text-left">For more information, please take a look at the assignment page  <a href="https://cei-lab.github.io/ece3400-2018/lab4.html">here</a>.</p>

              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- MILESTONES ------------------------------------------------------------------------------>
    <!-- Portfolio Modal 5 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-5">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
             <h2 class="text-secondary text-uppercase mb-0">Milestone 1</h2>
              <hr class="star-dark mb-5">
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-3 text-left">
                The goal of Milestone 1 was to add line tracking and maze traversing functionality to the robot in the absence of walls and treasures. The end goal was to have a robot capable of following a figure eight pattern on the ground. We added the following additional materials:
              </p>
              <ul class="mb-3 text-left">
                <li>3 QRE1113 Sensors on SparkFun breakout boards (https://www.sparkfun.com/products/9453)</li>
                <li>A breadboard</li>
                <li>A new 3D printed base for our robot</li>
                <li>3D printed connector parts</li>
                <li>White (or black) electrical tape</li>
                <li>More wires</li>
              </ul>
              <img class="img-fluid mb-5" src="img/Robot2.jpg" alt="Our revamped robot">
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Line Detection</h4>
              <p class="mb-5 text-left">The robot needed to be able to navigate lines made of the electrical tape on a flat surface arranged in a grid pattern. In order to be able to detect where lines were on the ground, we used QRE1113 analog sensors. These were connected to an analog input pin on the Arduino which gave a value based on the reflectivity of nearby surfaces using IR, and were powered from the same 5V and GND powering the Arduino.<br />
              <br />
              We then moved the line sensor above the ground surface at various heights and above both the taped line and the adjacent surface in order to discover some baseline values for what reflectivities mean what surface. We used those baseline values to determine a threshold, below which meant that the line sensor was on a line, and above which the opposite was true. We could now tell whether or not a line sensor on our robot was above a line!
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Line Following</h4>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/DCLGwZAPSa0?rel=0" frameborder="0" allow="autoplay; encrypted-media" class="mb-3" allowfullscreen></iframe>
              <p class="mb-3 text-left">We used three sensors (left, center, and right) to figure out where the robot was at any point in time. These sensors were placed at the front of the robot, spaced so that the distance between the left and right sensors is approximately the width of the line. The robot is considered centered when only the center sensor is on the line. When the left and center sensors are on the line, then the robot is veering slightly right, and we must turn slightly left. Similarly, when the right and center sensors are on the line, then the robot is veering slightly left and we must turn slightly right. If only the left sensor is on the line, then the robot is veering too much to the right and we must turn left more quickly. Similarly, if only the right sensor is on the line, the robot is veering too much to the left and we must turn right more quickly.<br />
              <br />
              To detect an intersection, the analog value of each sensor is read using the analogRead function and then each value is compared to a light threshold. For example, when both the left AND right sensors read below the threshold value, meaning that both are directly above lines, we have determined that the robot is at an intersection and continue straight through the intersection in order to follow the line.</p>
              <pre class="mb-0 text-left">
                <code>
void loop() {
    // Our bot repeatedly moves forward then corrects itself
    forward();
    linefollow();
}

void forward(){
    // Moves the bot forward
    MotorLeft.write(83);
    MotorRight.write(95);
    delay(100);
}

void linefollow(){
    //Below 950 is white tape
    //Above 950 is dark
    LightDataC = analogRead(LightCenter);
    LightDataL = analogRead(LightLeft);
    LightDataR = analogRead(LightRight);
    if (LightDataC <= 950 && LightDataL > 950 && LightDataR > 950){
          // centered
          return;
    } else if (LightDataL <= 950 && LightDataR <= 950) {
          // intersection
          return;
    } else if (LightDataC <= 950 && LightDataL <= 950){
          // bot is veering right slightly, so we turn it left a bit
          MotorRight.write(92);
          MotorLeft.write(80);
          delay(400);
          return;
    } else if (LightDataC <= 950 && LightDataR <= 950){
          // bot is veering left slightly, so we turn it right a bit
          MotorRight.write(100);
          MotorLeft.write(88);
          delay(400);
          return;
    } else if (LightDataL <= 950){
          // bot is veering right a lot, so we turn it left more
          MotorRight.write(92);
          MotorLeft.write(80);
          delay(400);
          return;
    } else if (LightDataR <= 950){
          // bot is veering left a lot, so we turn it right more
          MotorRight.write(100);
          MotorLeft.write(88);
          delay(400);
          return;
    } else {
         // this case should never be reached!! if it does then y i k e s
    }
}
                </code>
              </pre>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Figure 8</h4>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/iWSI64gUKew?rel=0" frameborder="0" allow="autoplay; encrypted-media" class="mb-3" allowfullscreen></iframe>
              <p class="mb-0 text-left">To accomplish a figure 8 we used our line following code to determine when we are at an intersection. Since all of our IR line sensors are in the front we then continued forward for an additional 350ms to ensure that we would begin turning when the robot is in the center of the intersection. Since we had to turn left and right in a complex pattern, we then referenced an array of integers using a global incrementing variable as the index. If the array was a 1 at that point then we would turn left, if it was 0 we would turn right. To accomplish the turn, we spun around for approximately 500ms and then finished the turn using a while loop which uses our line following code to detect when we have aligned with the line. Once the turn completed we incremented our global variable and continued forward until the next intersection.<br />
              <br />
              At the top of our code, we initialized the following globals:
              </p>
              <pre class="mb-0 text-left">
                <code>
int turn[8] = {1,0,0,0,0,1,1,1};
int i = 0;
                </code>
              </pre>
              <p class="mb-0 text-left">
                We added a push button on our robot so that it waits in setup() until we push the button:
              </p>
               <pre class="mb-0 text-left">
                <code>
// The following code is all in setup():
pinMode(8, INPUT);
...
while(digitalRead(8) !=  HIGH);
                </code>
              </pre>
              <p class="mb-0 text-left">
                Our main loop moves the robot forward then corrects itself if necessary. While it's correcting itself, if it is at an intersection, it turns. We use turnRight() and turnLeft() as helper methods to turn the robot.
              </p>
              <pre class="mb-0 text-left">
                <code>
void loop() {
    // put your main code here, to run repeatedly:
    forward();
    linefollow();
    delay(20);
}

void turnRight(){
    MotorLeft.write(80);
    MotorRight.write(80);
    delay(600);
    while(!(LightDataC <= 775 && LightDataL > 775 && LightDataR > 775)){
      LightDataC = analogRead(LightCenter);
      LightDataL = analogRead(LightLeft);
      LightDataR = analogRead(LightRight);
    }
    MotorLeft.write(90);
    MotorRight.write(90);
    delay(100);
    return;
}

void turnLeft(){
    MotorLeft.write(100);
    MotorRight.write(100);
    delay(600);
      while(!(LightDataC <= 775 && LightDataL > 775 && LightDataR > 775)){
      LightDataC = analogRead(LightCenter);
      LightDataL = analogRead(LightLeft);
      LightDataR = analogRead(LightRight);
    }
    MotorLeft.write(90);
    MotorRight.write(90);
    delay(100);
    return;
}
                </code>
              </pre>
              <p class="mb-0 text-left">
                The linefollow function follows the same pattern as before, but we now navigate at intersections as follows:
              </p>
              <pre class="mb-0 text-left">
                <code>
if (LightDataL <= 775 && LightDataR <= 775) {
  // Intersection
  digitalWrite(LED_BUILTIN, HIGH);
  forward();
  delay(350);
  if (turn[i] == 1){
      turnRight();
  }
  else turnLeft();
  if (i ==7){
    i = 0;
  }
  else {
        i = i + 1;
  }
  digitalWrite(LED_BUILTIN, LOW);
  return;
}
                </code>
              </pre>
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Milestone1">View Milestone 1 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Milestone 1</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 6 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-6">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Milestone 2</h2>
              <hr class="star-dark mb-5">
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-5 text-left">
                The goal of Milestone 2 was to add right-hand wall-following and robot-detecting functionality to the robot while maintaining previously built line-tracking capabilities. To detect walls, we used IR distance sensors, one on the front of the robot and one on the right. We also added additional LEDs (in series with 330ohm resistors) in order to signal what the robot is thinking as it progress through the maze.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Robot Redesign</h4>
              <p class="mb-5 text-left">
                Many parts of our circuitry from previous labs and milestones were scattered across multiple breadboards and had to be powered by external power sources. Therefore, in order to have a contained drivable robot we needed to compactify our circuits and transfer them onto the main robot body. No major changes were made to either the IR detection or microphone circuit, although many of the specific pins we read have changed. We will continue to compact our circuit and make the connections more permanent/reliable throughout the rest of the lab sections. Note that for this milestone, we disconnect the microphone circuit entirely in order to make use of as many analog input pins as possible. In the future, we intend to free up analog inputs by replacing their usage with muxing or digital triggering circuits.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Circling a set of walls through right-hand wall-following</h4>
              <p class="mb-5 text-left">
                  As shown in the video below, the red LED turns on when the robot senses a wall in front of it, and the green LED turns on when it senses a wall to the right. These LEDs only change at an intersection in the grid, because we only check for walls at these intersections. We determined a threshold value for the distance sensors by holding a wall a certain distance away from the robot so that we don't detect a wall too early or too late. Our right-hand wall-following logic is determined through a series of if-statements.
              </p>
              <p class="mb-5 text-left">
                Firstly, in order to follow the wall to the right, we always want to turn right if there is no wall. Otherwise, if there is a wall to the right and no wall in front, we follow the wall forwards. Finally, if there are walls both the front and right, then we try to find a direction which does not have a wall in front by turning left. Each turn rotates the vehicle 90 degrees, so that once we know that there is no longer a wall both to the front and to the right, then there should be no wall to the front and we can continue driving forward. This allows us to reliably follow walls regardless of the configuration.
              </p>
              <iframe class="mb-5" width="560" height="315" src="https://www.youtube.com/embed/R7MomRgZwi8?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <p class="mb-0 text-left">The following is commented code on how we implemented this:</p>
              <pre class="mb-0 text-left">
                <code>
// this is some code from our linefollow() method from Milestone 1 that we changed for this milestone
void linefollow(){
    //Below LIGHTTHRESHOLD is white tape
    //Above LIGHTTHRESHOLD is dark
    LightDataC = analogRead(LightCenter);
    LightDataL = analogRead(LightLeft);
    LightDataR = analogRead(LightRight);

    bool leftOnLine = LightDataL <= LIGHTTHRESHOLD;
    bool centerOnLine = LightDataC <= LIGHTTHRESHOLD;
    bool rightOnLine = LightDataR <= LIGHTTHRESHOLD;

    if (centerOnLine && !leftOnLine && !rightOnLine) {
        // centered
        Serial.println("Centered");
        return;
    } else if (leftOnLine && rightOnLine) {
        forward();
        delay(650); // this allows the robot to be centered on top of the intersection before turning
        wallfollow(); // new method!
        Serial.println("intersection");
        return;
    }
    ... // other cases
}

// our wall-following code
void wallfollow(){
  wallRight = analogRead(A5);
  wallFront = analogRead(A4);
  if (wallRight >= SOMETHRESHOLD) digitalWrite(rightWallLED, HIGH); else digitalWrite(rightWallLED, LOW);   // turn the right wall LED on
  if (wallFront >= SOMETHRESHOLD) digitalWrite(frontWallLED, HIGH); else digitalWrite(frontWallLED, LOW);   // turn the front wall LED on
  if (wallFront <= SOMETHRESHOLD && wallRight >= SOMETHRESHOLD) { // if greater than threshold there is a wall
      // following the right wall: we can go straight
      return;
  }
  if (wallRight <= SOMETHRESHOLD){  // nothing on the right, so we can turn right
      turnRight(); // turns right until the center line sensor hits a line
      return;
  }
  while (wallFront >= SOMETHRESHOLD && wallRight >= SOMETHRESHOLD){ // blocked on both front and right- keep turning until we aren't blocked
      turnLeft(); // turns left until the center line sensor hits a line
      wallRight = analogRead(A5);
      wallFront = analogRead(A4);
      // signal our findings with LED
      if (wallRight >= SOMETHRESHOLD) digitalWrite(rightWallLED, HIGH); else digitalWrite(rightWallLED, LOW);
      if (wallFront >= SOMETHRESHOLD) digitalWrite(frontWallLED, HIGH); else digitalWrite(frontWallLED, LOW);
  }
  return;
}
                </code>
              </pre>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Avoiding other robots</h4>
              <p class="mb-5 text-left">
                In order to be able to detect and avoid other robots, we needed to integrate the IR FFT detection with the other analog input reading that our Arduino does. However, we quickly found that the FFT detection code from Lab 2 changed some of the serial communication registers and timers, which results in servos behaving strangely or not moving at all. In order to get around this obstacle, we stored the state of the required registers and then restore them once the FFT is processed and we return: TIMSK0, ADCSRA, ADMUX, and DIDR0.
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Milestone 2 Photos/Noise_5V.jpg" alt="Noise on Arduino 5V"></img>
                <figcaption>Figure 2: Noise Out of 5V Pin</figcaption>
              </figure>
              <p class="mb-5 text-left">
                We also found in our testing that our robot was failing to reliably detect walls once powered off of the Arduino's 5V line, due to coupling with other components or the operations within the Arduino. This meant that a single reading from the IR FFT code was not sufficient to ascertain that a robot was present, but the sensing was also not reliable enough that we could count on every single reading returning that there was a robot present. Therefore, we needed to have some way of keeping track of how often we thought there was a robot present. We required that several readings detect the presence of a 6kHz IR source in quick succession by having count of the number of readings that increases when the FFT says there is a 6kHz peak, and otherwise slowly decays. Then, we only tried to avoid robots when the reading count was high enough. This essentially allows us to filter through the noise.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Line-tracking & avoiding robots and walls</h4>
              <p class="mb-5 text-left">
                We then were able to reliably do line tracking, wall avoidance, and robot avoidance together.
              </p>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/XvThMN8REoo?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <hr class="mb-5">
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Milestone2">View Milestone 2 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 7 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-7">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Milestone 3</h2>
              <hr class="star-dark mb-5">
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-5 text-left">
                The goal of Milestone 3 was to make our robot capable of complete maze exploration using a search algorithm (or a combination) from the following common algorithms: DFS, BFS, Dijkstra, or A* search. We decided to use a simple DFS for a more safe, easy-to-debug graph traversal after failing to incorporate a greedy search into our DFS due to problems with pointers, but we will continue to try incorporating the greedy search!
              </p>
              <hr class="mb-5">
              <p class="mb-5 text-left">
                To quickly demonstrate the goal of this milestone, here is a quick video of our robot doing a DFS exploration on a small (3x2) grid:
              </p>
              <iframe class="mb-5" width="560" height="315" src="https://www.youtube.com/embed/D40u1aN-03o?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">DFS</h4>
              <p class="mb-3 text-left">
                Here is an image that gives an overview of how DFS works, taken from <a href="https://www.hackerearth.com/practice/algorithms/graphs/depth-first-search/tutorial/">here</a>: 
              </p>
              <img class="img-fluid mb-3" style="max-height:300px" src="img/Milestone3Photos/DFS.jpg" alt="DFS explained visually">
              <p class="mb-3 text-left">
                We use a recursive strategy for our DFS. At each location, we call <code>dfs()</code> on all the possible locations we can move to from the current location, so that they are all one step away in one of the four (N, S, E, W) directions. We only call <code>dfs()</code> on valid exploration locations, i.e. locations that are in the grid and have not been explored. This way we don't create an enormous call stack. Before we actually move to a location, we check if we shouldMove() (no walls blocking us). 
              </p>
              <p class="mb-0 text-left">The following code snippet illustrates our DFS algorithm:</p>
              <pre class="mb-0 text-left">
                <code>
StackArray&ltuint8_t&gt path; // maintains the current path

void dfs(uint8_t xCoor, uint8_t yCoor) { // xCoor and yCoor are the next location to move to, guaranteed to be one away, a valid location, and not explored
  if (shouldMove(xCoor, yCoor)) { // check if a wall is blocking our way- if so, we shouldn't move there.
    moveOne(xCoor, yCoor); // this function physically moves the robot one step to (xCoor, yCoor), updating the current location
    path.push(yCoor); // add the y coordinate to the stack (this is the current lcoation)
    path.push(xCoor); // add the x coordinate to the stack (this is the current lcoation)
    explore(); // old code that reads sensors to explore the location for walls and treasures
    if (explored == rows*columns){
      // we've explored the entire maze!
      while(1){
        // stops moving
        MotorLeft.write(90);
        MotorRight.write(90);
      }
    }
    // here we call dfs() recursively on the 4 possible ways to move, but only if it's a valid location
    // our call order for the 4 directions is different based on the current direction- we show one of our if-statements below
    if (current_dir == S) {
        // only call dfs() for locations that haven't been explored and are on the grid to minimize call stack
        if (maze[xCoor+1][yCoor] == 0 && xCoor+1 >= 0 && yCoor >= 0 && xCoor+1 < rows && yCoor < columns) {
          dfs(xCoor+1, yCoor);
        }
        if (maze[xCoor][yCoor-1] == 0 && xCoor >= 0 && yCoor-1 >= 0 && xCoor < rows && yCoor-1 < columns) {
          dfs(xCoor, yCoor-1);
        }
        if (maze[xCoor][yCoor+1] == 0 && xCoor >= 0 && yCoor+1 >= 0 && xCoor < rows && yCoor+1 < columns) {
          Serial.println("made it");
          dfs(xCoor, yCoor+1);
        }
        if (maze[xCoor-1][yCoor] == 0 && xCoor-1 >= 0 && yCoor >= 0 && xCoor-1 < rows && yCoor < columns) {
          dfs(xCoor-1, yCoor);
        }
    } 
    // ... and so on, with a different order for each of the other 3 directions
    // at this point, if we get here, this means we hit a dead end, so we backtrack
    // remove current loc from stack
    path.pop();
    path.pop();
    // get the previous location to move to- just a peek
    uint8_t backX = path.pop();
    uint8_t backY = path.peek();
    path.push(backX);
    // move to the location- it is already guaranteed reachable
    moveOne(backX, backY);
  }
}
                </code>
              </pre>
              <p class="mb-0 text-left">
                As seen above, to make our search easier to implement, we imported a library called <code>StackArray</code> to maintain a stack of our current path, which allows us to easily backtrack when we hit a dead end. All we do is pop the current location off the stack (we pop twice, once for the x-coordinate and once for the y-coordinate), and then peek at the next two coordinates to get the location we want to move to. The invariant is that the location we move to is always one step away and is physically reachable from the current location, i.e. there are no walls blocking our path. <br />
                <br />
                We start the DFS in <code>loop()</code> by exploring the first (0, 0) location, then adding (0, 0) to the path stack, and then calling dfs() twice on the south and east locations, because those are the only two that are valid based on our starting location and direction (south). The code is below:
              </p>
              <pre class="mb-0 text-left">
                <code>
void loop() {
  explore();
  path.push(y);
  path.push(x);
  dfs(1, 0);
  uint8_t backX = path.pop();
  uint8_t backY = path.pop();
  moveOne(backX, backY); // move back to the starting location to branch to the other side of DFS
  dfs(0, 1);
  // we should have explored the entire maze at this point- this is just for debugging
  Serial.println("somehow finished");
  while(1){}
}
                </code>
              </pre>
              <p class="mb-5 text-left">
                We've run this algorithm on two different 4x5 grids- one is more simple, one is quite complex. The following two videos show the results. One quick note is that in the videos, we correct the robot's course quite a few times due to the robot missing the lines (we filmed during sunrise so the light kept changing, and different parts of the maze were lighter than others). Even so, it is easy to prove that our algorithm is still correct, because once we put the robot on its intended path, it continues to explore smoothly, which means it knows where it is, and that indeed where we moved it is where it was intended to go.
              </p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/-l_l9L6xsmo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <iframe class="mb-5" width="560" height="315" src="https://www.youtube.com/embed/rsmMVJcRQ0Q?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Updating the GUI</h4>
              <p class="mb-5 text-left">
                The following are two videos of our robot updating the GUI as it explores the maze! The first video is a very small 3x2 maze to quickly show that the GUI works, and the second is the large 9x9 practice maze to show that the robot can complete the maze and correctly display the entire maze on the GUI. The GUI was implemented in Lab 3. Treasures are not included yet.
              </p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/WrjwwjlX1Lc?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/P19dxJDsDUs?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">At the Competition</h4>
              <p class="mb-5 text-left">
                After many hours we were able to write a correct and working DFS & greedy algorithm, but unfortunately after trying the robot on large mazes (ones with >40 tiles) it started to reset itself due to lack of dynamic memory. Unable to condense our code any more, we decided that for the competition we would use our simple DFS, which is able to cover the entire maze for sure. This proved to be good enough for the competition, as we ended up winning first place!! :)
              </p>
              <hr class="mb-5">
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Milestone3">View Milestone 3 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 8 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-8">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Milestone 4</h2>
              <hr class="star-dark mb-5">
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-5 text-left">
                The goal of Milestone 4 was to add shape detection functionality to the robot, adding upon the progress made in Lab 4. In order to do this, we employed a number of different techniques, before settling upon generally using edge detection in order to determine the shape of the treasure. We use the same OV7670 and DE0-NANO from Lab 4, but with an updated communication protocol and different software.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">The Switch to 444</h4>
              <p class="mb-3 text-left">
                As we continued to interface with the OV7670, we found that the camera tended to have streaking effects and visual artifacts. Although these were mitigable by image processing techniques, occasionally red shapes would appear entirely blue and vice versa, rendering our color detection algorithms useless. It is possible that the high speed (up to 24MHz) signals were above the actual Arduino or camera specifications and led to undefined behavior.
              </p>
              <p class="mb-3 text-left">
                  We found that switching our color data format from RGB 565 to RGB 444, in which each color field contains 4 bits of information, kept the same amount of color data once downsampled, but lead the much more stable and accurate color information. A snippet from the new downsampling algorithm is below.
              </p>
              <pre class="mb-1 text-left"><code>
if (CAM_COUNT == 1'b0)begin
  W_EN = 1'b0; // on the first cycle of a pixel - do not write!
  data[7:0] = CAM_DATA; // shift data from camera into LSByte
  CAM_COUNT = 1'b1; // next, we take in the MSByte data
  X_ADDR = X_ADDR;
end
else begin
  data[15:8] = CAM_DATA; // shift data from camera into MSByte
  CAM_COUNT = 1'b0; // next, go to LSByte

  downsampled[7:5] = data[11:9]; // downsample
  downsampled[4:2] = data[7:5]; // data takes form X R G B
  downsampled[1:0] = data[3:2]; // where each is 4 bits

  X_ADDR = X_ADDR + 1; // we now go to next address
  W_EN = 1'b1; // write downsampled to RAM
end
              </code></pre>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Additional Color History</h4>
              <p class="mb-3 text-left">
                  Reliably determining shape requires some form of comparative information - is there an edge at a particular location? Does the shape get wider or narrower as you move down across an image? To achieve accurate edge detection, we store pixel data for the current row as well as two previous rows of the image. In particular, we store whether or not we believed that a particular pixel was blue or red. This allows to determine the presence of the edge of a shape, as well as what direction the edge is in. 
              </p>
              <p class="mb-5 text-left">
                  Since we store data from the previous three rows, we are able to search for matches with a 3x3 pixel pattern which represent an edge. In particular, we found that comparing with four particular patterns were useful in determining treasure shape. As seen in the snippet below, we look for approximately diagonal edge patterns in the pixels. A similar code snippet would check for red diagonal patterns.
              </p>
              <pre class="mb-1 text-left"><code>
// blue_recent contains 3x3 grid of pixel color data
case (blue_recent)
  9'b111_110_100: begin
    // upper left
    num_diag_u_b = num_diag_u_b + 12'd1;
  end
  9'b111_011_001: begin
    // upper right
    num_diag_u_b = num_diag_u_b + 12'd1;
  end
  9'b001_011_111: begin
    // bottom right
    num_diag_d_b = num_diag_d_b + 12'd1;
  end
  9'b100_110_111: begin
    // bottom left
    num_diag_d_b = num_diag_d_b + 12'd1;
  end
endcase
              </code></pre>

              <p class="mb-5 text-left">
                  We then utilize this edge detection to differentiate between the shape types. In particular, slanted edges are a strong indicator of triangles and diamonds. As the diagram below indicates, triangles tend to have edge patterns with the colored pixels in the lower half, whereas diamonds are an even mix of upper and lower diagonals, and squares have very few of either. A predefined threshold would then separate noise from meaningful indicators of the presence of a triangle or diamond.
              </p>
              <img class="img-fluid mb-5" style="max-height:300px" src="img/Milestone4Photos/edgedetect.jpg" alt="Showcasing square, triangle, diamond edge patterns">
              <p class="mb-5 text-left">
                  We also found that while our color and shape detection usually remained accurate, there were still frequent blips on the information received by the Arduino. We combatted this by adding simple low-pass filters over many of the counting signals inside of the image processor module. This averaging spreads out the effect of outlier frames and reducing the chance that the Arduino will randomly sample an output which does not accurately represent the shape that the FPGA tends to believe the shape is. One example assignment for a low pass filter is below.
              </p>
              <pre class="mb-1 text-left"><code>

num_diag_u_r_p = (num_diag_u_r_p - (num_diag_u_r_p >> 3) ) + (num_diag_u_r >> 3);
 
Updated Arduino Communication
With the addition of treasure data, we now required a more advanced form of communication with the Arduino. We found that since we already had a 3-bit encoding for treasure data which was present in our Arduino GUI updating code and maze information storage, that it was natural to extend this to also be the format that the FPGA to Arduino communication took place. Instead of a pair of wires, now three wires run from the FPGA to the Arduino, each one being a bit in the three-bit treasure representation established in Lab 3. Below is the code running on the robot Arduino which reads the FPGA communication pins to determine treasure information.
 
  int FPGA_read = 0;
 
  FPGA_read += (digitalRead(FPGA_PIN_2) << 2);
  FPGA_read += (digitalRead(FPGA_PIN_1) << 1);
  FPGA_read += (digitalRead(FPGA_PIN_0) << 0);
  Serial.print(F("Received "));
  Serial.println(FPGA_read, BIN);
 
  switch (FPGA_read) {
    case 0:
      break;
    case 1:
      Serial.println(F("blue square"));
      break;
    case 2:
      Serial.println(F("red square"));
      break;
    case 3:
      Serial.println(F("blue dia"));
      break;
    case 4:
      Serial.println(F("red dia"));
      break;
    case 5:
      Serial.println(F("blue tr"));
      break;
    case 6:
      Serial.println(F("red tr"));
      break;
    default:
      Serial.println(F("Unknown"));
      break;
  }
</code></pre>
              <hr class="mb-5">
              <p class="mb-3">
                Two videos below showcase the robust color and shape detection. The first runs through all six types of treasures present in the final maze, and the second displays the FPGA's ability to differentiate between having a treasure and not having a treasure.
              </p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/06qw0oAm5l0?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/MnPnyvoe0og?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
  			  <hr class="mb-5">
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Milestone4">View Milestone 4 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Contract Modal -->
    <div class="portfolio-modal mfp-hide" id="contract-modal">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Team 28 Contract</h2>
              <hr class="star-dark mb-5">
              <div class="contract-content">
                <h3>Team Members</h3>
                <p class="mb-5">Glenna Zhang, Liliet Sosa, Anthony Viego, Kevin Ying</p>
                <hr>
                <h3>Team Procedures</h3>
                <ol class="mb-5">
                  <li><b>Day, time, and place for regular team meetings:</b><br />
                  UH142, Friday 11.15-12.05pm, weekly (We will allocate more time towards the end of the semester, as determined during weekly meetings).</li>
                  <li><b>Preferred method of communication (e.g., e-mail, cell phone, wired phone, Blackboard Discussion Board, face-to-face, in a certain class) in order to inform each other of team meetings, announcement, updates, reminders, problems:</b><br />
                  Messenger and team meetings. Extra team meeting times will be communicated through Google Calendar.</li>
                  <li><b>Decision-making policy (by consensus? by majority vote?):</b><br />
                  We will try to make major design decisions by consensus. Major policy disagreements should allow for a waiting time, preferably overnight, before a final decision is made. If the team is still evenly split between decisions, the team will ask for the opinion of an instructor, and if necessary the instructor will have final say.</li>
                  <li><b>Method for setting and following meeting agendas (Who will set each agenda? When? How will team members be notified/reminded? Who will be responsible for the team following the agenda during a team meeting? What will be done to keep the team on track during a meeting?):</b><br />
                  The current team leader will take charge of setting the agenda, notifying team members, and keeping the team on track. Agendas will be written in our team’s shared Google Drive folder.</li>
                  <li><b>Method of record keeping (Who will be responsible for recording & disseminating minutes? How & when will the minutes be disseminated? Where will all agendas & minutes be kept?):</b><br />
                  The current team leader will be in charge of taking minutes. The minutes will be in our team’s shared google drive.</li>
                </ol>
                <hr>
                <h3>Team Expectations</h3>
                <div class="mb-5">
                  <h5>Work Quality</h5>
                  <ol>
                    <li><b>Project standards (What is a realistic level of quality for team presentations, collaborative writing, individual research, preparation of drafts, peer reviews, etc.?):</b><br />
                    The team will review and agree the level of the work is acceptable.</li>
                    <li><b>Strategies to fulfill these standards:</b>
                      <ul>
                        <li>Communication - standards should be clear at the start</li>
                        <li>Working hard</li>
                        <li>Team work!!</li>
                      </ul>
                    </li>
                  </ol>
                  <h5>Team Participation</h5>
                  <ol>
                    <li><b>Strategies to ensure cooperation and equal distribution of tasks:</b>
                      <ul>
                        <li>Team will agree on the division of tasks</li>
                        <li>Do your assigned task</li>
                        <li>Communicate with team members when encountering difficulties, even when this means possibly having to reassign tasks</li>
                      </ul>
                    </li>
                    <li><b>Strategies for encouraging/including ideas from all team members (team maintenance):</b>
                      <ul>
                        <li>Make sure everyone’s voices are being heard</li>
                        <li>Welcoming team environment</li>
                        <li>Ask other team members often for their ideas</li>
                      </ul>
                    </li>
                  <li><b>Strategies for keeping on task (task maintenance):</b>
                    <ul>
                      <li>Team leader makes sure agenda is being followed during meetings</li>
                      <li>Check on team members’ progress at the start of each meeting</li>
                      <li>Sub-teams/whole team schedules blocks of time to work together on assigned tasks</li>
                    </ul>
                  </li>
                  <li><b>Preferences for leadership (informal, formal, individual, shared):</b><br />
                    We want to keep leadership informal throughout the whole semester to make sure that the team atmosphere is positive. Since we will be rotating who the team leader is, it will be an individual responsibility.
                  </li>
                  </ol>
                  <h5>Personal Accountability</h5>
                  <ol>
                    <li><b>Expected individual attendance, punctuality, and participation at all team meetings:</b>
                      <ul>
                        <li>Members are expected to attend all meetings, be punctual and participate unless a valid reason is presented.</li>
                        <li>Members are expected to notify the team when conflicts occur.</li>
                      </ul>
                    </li>
                    <li><b>Expected level of responsibility for fulfilling team assignments, timelines, and deadlines:</b>
                      <ul>
                        <li>Members are expected to complete their assignments on-time.</li>
                        <li>Members are expected to create their own timelines and request help from other members to ensure that the work is completed.</li>
                      </ul>
                    </li>
                    <li><b>Expected level of communication with other team members:</b>
                      <ul>
                        <li>Members are expected to be active participants in face-to-face meetings and in online communication.</li>
                      </ul>
                    </li>
                    <li><b>Expected level of commitment to team decisions and tasks:</b>
                      <ul>
                        <li>Members are expected to be committed to a team decision and should not ignore team decisions to do things “their way”</li>
                        <li>Members are expected to vocalize any doubts or opposing opinions in order for the team to be able to come to a happy consensus</li>
                      </ul>
                    </li>
                  </ol>
                  <h5>Consequences for Failing to Follow Procedures and Fulfill Expectations</h5>
                  <ol>
                    <li><b>Describe, as a group, you would handle infractions of any of the obligations of this team contract:</b>
                      <ul>
                        <li>First infractions will result in a verbal warning.</li>
                        <li>Further infractions will result in a team discussion of the behavior</li>
                      </ul>
                    </li>
                    <li><b>Describe what your team will do if the infractions continue:</b>
                      <ul>
                        <li>Infracting members will have to take meeting minutes in place of the team leader.</li>
                        <li>Continued infractions will result in buying the team boba.</li>
                      </ul>
                    </li>
                  </ol>
                </div>
                <hr>
                <h3>Team Leadership</h3>
                <p class="mb-5">
                  <b>Every person on the team will have to take the role as a leader. The role of the leader will be to organize meetings and make sure that everything is submitted in a timely manner. Please note here who will be responsible when:</b><br />
                  <br />
                  Weeks 1&2: Kevin Ying<br />
                  Weeks 3&4: Glenna Zhang<br />
                  Weeks 5&6: Liliet Sosa<br />
                  Weeks 7&8: Anthony Viego<br />
                  Weeks 9&10: Glenna Zhang<br />
                  Weeks 11&12: Liliet Sosa<br />
                  Weeks 13&14: Kevin Ying<br />
                  Weeks 15&16: Anthony Viego<br />
                </p>
                <hr>
                <div class="mb-5">
                  <b>
                    I participated in formulating the standards, roles, and procedures as stated in this contract.<br />
                    I understand that I am obligated to abide by these terms and conditions.<br />
                    I understand that if I do not abide by these terms and conditions, I will suffer the consequences as stated in this contract.<br />
                  </b>
                  <br />
                  <ol>
                    <li>Anthony Viego, 8/31/18</li>
                    <li>Kevin Ying, 8/31/18</li>
                    <li>Glenna Zhang, 8/31/18</li>
                    <li>Liliet Sosa, 8/31/18</li>
                  </ol>
                </div>
              </div>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Ethics Modal -->
    <div class="portfolio-modal mfp-hide" id="ethics-modal">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Team 28: Ethics Assignment</h2>
              <hr class="star-dark mb-5">
              <h4 class="text-left">Question</h4>
              <h5 class="mb-5 text-left">Is “a ban on offensive autonomous weapons beyond meaningful human control” going to work?</h5>
              <hr class="mb-5">
              <h4 class="text-left">Response</h4>
              <p class="mb-3 text-left">
                A ban on offensive autonomous weapons would be beneficial to humanity without a doubt; however, it would ultimately not be very effective. These weapons are very cheap to make and they do not require hard-to-obtain materials (1). This means that although most scientists agree that such weapons would be detrimental to society, unsupervised groups, such as terrorist groups, could very easily develop and use their own autonomous weapons. To counter these types of attacks, the rest of society would be pushed to develop autonomous weapons as well. The ban would be a good start, but much work would need to be done to make it effective.
              </p>
              <p class="mb-3 text-left">
                Furthermore, the definition of what weapons are included in this ban is unclear and allows for quite a grey area. The specific wording of the ban is weapons that do not have “meaningful human control” but one could argue that a facial recognition weapon has meaningful human control. As an example, a quadcopter with facial recognition could still be flown by a pilot and the target selected by the pilot, but the device simply locks onto the target once it is found and attacks. Even if the quadcopter chooses its target independently, a human must have programmed how to handle target selection, therefore adding human bias and a form of “human control” to the weapon’s algorithm. Additionally, it would not be hard for groups to simply conceal their research under another name or idea. Especially in the case of autonomous weapons and robotics it would be fairly easy for a group to claim they are working on a search/rescue vehicle but really developing the framework and infrastructure for an autonomous weapon.
              </p>
              <p class="mb-3 text-left">
                While most countries are likely to follow the ban as can be seen in the case of the ban on chemical warfare, there are always countries that will ignore it. To rectify this issue a governing body would be needed to oversee the ban. The most likely candidate for this would be the UN’s ICJ. However, this organization works relatively slow and technological development is notably fast. To implement this ban correctly governments around the world would need to create individual bodies responsible for preventing the creation of such weapons. However, whether or not they will do so is unclear. Many will also attempt to make an argument similar to a “nuclear deterrent” in which everyone should have autonomous weapons, so no one uses them. Yet, in this case, that most likely will not work as nuclear bombs and autonomous weapons are in very different leagues and it is entirely possible for the use of autonomous weapons to even go unnoticed. 
              </p>
              <p class="mb-3 text-left">
                Overall, a ban on autonomous weapons is very difficult to implement since there are many factors that world leaders cannot control that would contribute to its ultimate failure.
              </p>
              <p class="text-left" style="margin: 0px">
                <b>Sources:<b>
              </p>
              <p class="mb-3 text-left" style="margin-left: 40px">
                (1) "Autonomous Weapons: An Open Letter from AI & Robotics Researchers." <a href="https://futureoflife.org/open-letter-autonomous-weapons/?cn-reloaded=1">https://futureoflife.org/open-letter-autonomous-weapons/?cn-reloaded=1</a>
              </p>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
	<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
    <script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/freelancer.min.js"></script>



  </body>

</html>
