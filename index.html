<!DOCTYPE html>

<html lang="en">

  <head>
  <style>
  h2.ex1 {
    padding: 35px;
}
  h4.ex1 {
    padding: 10px;
	}
</style>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>:angry: - ECE3400</title>
	<link rel="icon" href="img/angry-face.png">

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

    <!-- Plugin CSS -->
    <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet" type="text/css">

    <!-- Custom styles for this template -->
    <link href="css/freelancer.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg bg-secondary fixed-top text-uppercase" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">:angry:</a>
        <button class="navbar-toggler navbar-toggler-right text-uppercase bg-primary text-white rounded" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item mx-0 mx-lg-1">
              <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#about">About</a>
            </li>
            <li class="nav-item mx-0 mx-lg-1">
              <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#team">The Team</a>
            </li>
            <li class="nav-item mx-0 mx-lg-1">
              <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#labs">Labs</a>
            </li>
            <li class="nav-item mx-0 mx-lg-1">
              <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#milestones">Milestones</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead text-white text-center">
      <div class="container">
        <img class="img-fluid mb-4 d-block mx-auto" src="img/angry-face.png" alt="The Facebook Angry React">
        <h1 class="text-uppercase mb-0">:angry:</h1>
        <hr class="star-light">
        <h2 class="font-weight-light mb-0">
          ECE 3400 Team 28 || Fall 2018
        </h2>
      </div>
    </header>

    <!-- About Section -->
    <section class="bg-primary text-white mb-0" id="about">
      <div class="container">
        <h2 class="text-center text-uppercase text-white">About</h2>
        <hr class="star-light mb-5">
        <div class="row">
          <div class="col-lg-4 ml-auto" id="video">
            <a href="https://youtu.be/Ex6m2g_6p3g">
              <img class="img-fluid mb-3 d-block mx-auto" src="img/Robot_About.jpg" alt="The Robot">
            </a>
            <p id="list">Click to see a video of our robot!</p>
          </div>
          <div class="col-lg-7 mr-auto">
            <p class="lead">Team 28, also known as :ANGRY: or the Angry Reacts, is the highest numbered - and highest quality - team in the ECE 3400 maze exploration robot competition.</p>
            <p>Our robot is fully autonomous and capable of line following, wall-following, tone detection, and IR detection and avoidance of other robots. In addition, we recently added the ability to communicate with a base station <a class="portfolio-item" href="#portfolio-modal-3">using an RF transceiver</a> to display the maze state to a display as we map it. We're currently working on <a class="portfolio-item" href="#portfolio-modal-4">integrating a camera and FPGA</a> to detect visual treasures. Our robots were built with an Arduino Uno, a DE0-nano FPGA, custom 3D-printed components, and various sensors and servos. To hear more about the competition, please take a look at the <a href="https://cei-lab.github.io/ece3400-2018/">ECE 3400 course page</a>.</p>
          </div>
        </div>
      </div>
    </section>
    
    <!-- Team Section -->
    <section class="bg-dark text-white mb-0" id="team">
      <div class="container">
        <h2 class="text-center text-uppercase text-white ex1">
          The Team
        </h2>
        <div class="row">
          <div class="col-sm-3 text-center">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="img/tony.png" alt="">
              <h4 class="ex1">Anthony Viego</h4>
              <h5 class="font-weight-light">Junior, ECE</h5>
            </div>
          </div>
          <div class="col-sm-3 text-center">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="img/Glenna.png" alt="">
              <h4 class="ex1">Glenna Zhang</h4>
              <h5 class="font-weight-light">Junior, ECE</h5>
            </div>
          </div>
          <div class="col-sm-3 text-center">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="img/profile.png" alt="">
              <h4 class="ex1">Liliet Sosa</h4>
              <h5 class="font-weight-light">Junior, ECE</h5>
            </div>
          </div>
          <div class="col-sm-3 text-center">
            <div class="team-member" id="kevin">
              <a href="https://kevinzying.github.io/resume/"><img class="mx-auto rounded-circle" src="img/Kevin.jpg" alt="A Picture of Kevin Ying at the One World Trade Center"></a>
              <h4 class="ex1">Kevin Ying</h4>
              <h5 class="font-weight-light">Junior, ECE</h5>
              <p class="mb-1 small" id="moreinfo">My focus so far has been on IR detection, radio communication, I2C for camera setup, and FPGA-Arduino communication. </p>
              <p class="mb-1 small" id="moreinfo">To check out more of my work, take a look at my online resume <a href="https://kevinzying.github.io/resume/">here</a>. </p>
              <p class="mb-1 small" id="moreinfo">I really just wanted to show off that I found this cool CSS transition on StackExchange. </p>
              <!-- Aforementioned stackexchange link: https://stackoverflow.com/a/30531678 -->
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Lab Grid Section -->
    <section class="portfolio" id="labs">
      <div class="container">
        <h2 class="text-center text-uppercase text-secondary mb-0">Labs</h2>
        <hr class="star-dark mb-5">
        <div class="row">
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-1">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="Lab1/img/lab1.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Microcontroller</p>
            <p class="mb-1 small" id="moreinfo">In which we discover the Arduino Uno and start to get a moving robot.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-2">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/IR_circuit.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Analog Circuitry and FFTs</p>
            <p class="mb-1 small" id="moreinfo">In which we create detect tones and high frequency IR pulses.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-3">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/Lab 3 Photos/GUI.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">System Integration and Radio Communication</p>
            <p class="mb-1 small" id="moreinfo">In which we talk to a GUI, and rediscover all of our sensors.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-4">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/cake.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">FPGA and Shape Detection</p>
            <p class="mb-1 small" id="moreinfo">In which we add a camera and learn the joys of visual processing.</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Milestone Grid Section -->
    <section class="portfolio bg-red text-white" id="milestones">
      <div class="container">
        <h2 class="text-center text-uppercase mb-0">Milestones</h2>
        <hr class="star-light mb-5">
        <div class="row">
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-5">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/robot2.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Line Tracking</p>
            <p class="mb-1 small" id="moreinfo">In which our robot learns to perform a figure eight.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-6">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/Milestone 2 Photos/wallsensor.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Wall Detection</p>
            <p class="mb-1 small" id="moreinfo">In which our robot learns to stop hitting itself.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-7">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/submarine.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Maze Exploration</p>
            <p class="mb-1 small" id="moreinfo">In which our robot learns to search for what it wants.</p>
          </div>
          <div class="col-md-6 col-lg-3" id="lab-hover">
            <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-8">
              <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                  <i class="fa fa-search-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/cabin.png" alt="">
            </a>
            <p class="mb-1" id="moreinfo">Treasure Detection</p>
            <p class="mb-1 small" id="moreinfo">In which our robot learns to tell triangles from squares.</p>
          </div>
        </div>
      </div>
    </section>

	  <footer class="bg-info footer text-center">
      <div class="container">
        <div class="row">
          <div class="col-md-6 mb-0 mb-lg-0">
		    <a class="btn btn-primary btn-lg btn-block" href="https://github.com/ECE3400Team28/website/">View Code on GitHub</a>
		    <a class="portfolio-item btn btn-secondary btn-lg btn-block" href="#contract-modal">View Team Contract</a>
		  </div>
        </div>
      </div>
    </footer>

    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
    <div class="scroll-to-top d-lg-none position-fixed ">
      <a class="js-scroll-trigger d-block text-center text-white rounded" href="#page-top">
        <i class="fa fa-chevron-up"></i>
      </a>
    </div>

    <!-- Portfolio Modals -->

    <!-- Portfolio Modal 1 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-1">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Lab 1: Microcontroller</h2>
              <hr class="star-dark mb-5">

              <h3 class="mb-4">Description</h3>
              <p class="mb-2 text-left">This lab introduces the Arduino Uno and Arduino IDE, and ends with the assembly of a basic robot performing autonomous driving. The lab takes approximately three hours, and requires the following <strong>materials</strong>:</p>
              <ul class="mb-5 text-left">
                <li class="li-center">1x Arduino Uno</li>
                <li class="li-center">1x USB A/B cable</li>
                <li class="li-center">2x Continuous Parallax Servos</li>
                <li class="li-center">1x Solderless breadboard</li>
                <li class="li-center">1x 1k-100k Potentiometer</li>
                <li class="li-center">1x LED, any color</li>
                <li class="li-center">Several 1k and 330 Ohm resistors</li>
              </ul>
              <hr>
              <div class="media">
				        <div class="media-left">
        				  <div class="embed-responsive video">
                      <iframe width="560" height="315" src="https://www.youtube.com/embed/ZAQedv_tLnw?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        			 	  </div>
			          </div>
      			    <div class="media-body">
      			      <h4 class="media-heading">Blinking an internal LED</h4>
      			      <span>After installing the <a href="https://www.arduino.cc/en/Main/Software">Arduino IDE</a> and any necessary drivers, we found the Blink.ino example provided under <mark>File</mark> >> <mark>Examples</mark> >> <mark>1.Basics</mark> >> <mark>Blink</mark>, which toggles the board’s built-in LED once per second.</span>
      			    </div>
			        </div>
      			  <hr>
      			  <div class="media mb-2">
      			    <div class="media-body">
      			      <h4 class="media-heading">Blinking an external LED</h4>
                  <span>In order to test the other digital pins provided on the Uno, we created a global variable named <code>LED_PIN</code>, and modified the existing code to use this pin. We then changed the number assigned to the variable to test each digital pin. The following example tests digital pin 7.</span>
      			    </div>
      			    <div class="media-right">
      		       	<div class="embed-responsive video">
    				        <iframe width="560" height="315" src="https://www.youtube.com/embed/-SuszBuhR4I?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    		 	  	    </div>
      			    </div>
      			  </div>
              <pre class="text-left">
                <code>
const short LED_PIN = 7;
                
// the setup function runs once when you press reset or power the board
void setup() {
  // initialize digital pin LED_PIN as an output.
  pinMode(LED_PIN, OUTPUT);
}
                
// the loop function runs over and over again forever
void loop() {
  digitalWrite(LED_PIN, HIGH);       // turn the LED on (HIGH is the voltage level)
  delay(1000);                       // wait for a second
  digitalWrite(LED_PIN, LOW);        // turn the LED off by making the voltage LOW
  delay(1000);                       // wait for a second
}
                </code>
              </pre>
      			  <hr>
      			  <div class="media mb-3">
      			  	<div class="media-left">
      		       	<div class="embed-responsive video">
      				      <iframe width="560" height="315" src="https://www.youtube.com/embed/9hQ8VjL0_DI?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      		 	  	  </div>
      			    </div>
      			    <div class="media-body">
                  <h4 class="media-heading">Reading the value of a potentiometer via the serial port</h4>
                  <span>For this portion of the lab we used a potentiometer to change the voltages on the analog input pin of the arduino, and then programmed the arduino to output these voltages to the serial monitor.</span>
      			    </div>
      			  </div>
              <p class="mb-2 text-left">To do this we started serial communication and then read the analog input using <code>analogRead(INPUT_PIN)</code>. It should be noted that this does NOT return the actual voltage.</p>
              <pre class="mb-0 text-left">
                <code>
int INPUT_PIN = A0;
                  
// the setup function runs once when you press reset or power the board
void setup() {
  Serial.begin(9600);
}
                  
// the loop function runs over and over again forever
void loop() {
  delay(100);
                  
  // Analog input read
  int input = analogRead(INPUT_PIN);
  Serial.println(input);
}
                </code>
              </pre>
              <p class="text-left mb-5">To set up the circuit we wired the output of the potentiometer to the Arudino’s A0(analog input) pin. We then supplied VCC to the wiper pin of the potentiometer and attached a 330 ohm resistor the the output of the potentiometer and GND to form a voltage divider. </p>
      			  <hr>
      			  <div class="media mb-3">
      			    <div class="media-body">
      			      <h4 class="media-heading">Mapping values to the LED</h4>
      			      <span>To simulate and display an analog output, we connected an LED in series with a 330 ohm resistor to a digital pin with PWM capability (pin ~11). Then, we used our code for the potentiometer to pass its value as an input to the <code>analogWrite</code> function, along with the pin connected to the LED. </span>
      			    </div>
      			    <div class="media-right">
      		       	<div class="embed-responsive video">
      				      <iframe width="560" height="315" src="https://www.youtube.com/embed/3bNpFlZQYro?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      		 	  	  </div>
      			    </div>
              </div>
              <p class="mb-2 text-left">We had to scale the potentiometer value because <code>analogWrite</code> only writes values from 0 to 255, whereas the 10-bit analog input results in a value from 0 to 1023. Different values for the potentiometer changed the brightness of the LED. The code is shown below.</p>
              <pre class="mb-0 text-left">
                <code>
const short LED_PIN = 11;
int INPUT_PIN = A0;

// the setup function runs once when you press reset or power the board
void setup() {
  // initialize digital pin LED_PIN as an output.
  pinMode(LED_PIN, OUTPUT);
  Serial.begin(9600);
}

// the loop function runs over and over again forever
void loop() {
  delay(100);

  // Analog input read
  int input = analogRead(INPUT_PIN);
  Serial.println(input);
  
  // PWM
  analogWrite(LED_PIN, input/4);
}
                </code>
              </pre>
      			  <hr>
      			  <div class="media mb-3">
      			  	<div class="media-left">
      		       	<div class="embed-responsive video">
      				      <iframe width="560" height="315" src="https://www.youtube.com/embed/PLP5Jh7I-W8?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      		 	  	  </div>
      			    </div>
      			    <div class="media-body">
      			      <h4 class="media-heading">Mapping values to the servo</h4>
                  <span>We later used the value from the potentiometer to control a parallax servo. Similarly to the LED, we connected the servo to a PWM pin on the arduino (pin ~10). However, instead of using <code>analogWrite</code>, we imported the <code>Servo.h</code> library which takes care of setting the pin as an output and generating a proper waveform when we use its <code>attach()</code> and <code>write()</code> functions.</span>
      			    </div>
              </div>
              <p class="mb-2 text-left">Again, the values had to be scaled, this time to the range of 0 to 180 that the servo library takes in for its write function. We also added some initial code which demonstrates the servo moving at full speed in both directions, and at a stop.</p>
              <pre class="mb-0 text-left">
                <code>
const short SERVO_PIN = 10;
int INPUT_PIN = A0;
Servo angrySpin;

// the setup function runs once when you press reset or power the board
void setup() {
  Serial.begin(9600);
  angrySpin.attach(SERVO_PIN);
  angrySpin.write(0);
  delay(1000);
  angrySpin.write(90);
  delay(1000);
  angrySpin.write(180);
  delay(1000);
}

// the loop function runs over and over again forever
void loop() {
  delay(100);

  // Analog input read
  int input = analogRead(INPUT_PIN);
  Serial.println(input);

  // PWM
  angrySpin.write(input/6);
}
                </code>
              </pre>
      			  <hr>
      			  <div class="media mb-3">
      			    <div class="media-body">
      			      <h4 class="media-heading">Assembling our robot</h4>
      			      <span>To assemble our robot we used:</span>
                  <ul>
                    <li>A chassis</li>
                    <li>Two wheels</li>
                    <li>Several screws</li>
                    <li>A 9V battery</li>
                    <li>Ball bearing</li>
                    <li>Two servos</li>
                    <li>Arduino Uno</li>
                  </ul>
      			    </div>
      			    <div class="media-right">
      				    <img class="media-object img-fluid" src="img/Robot.png" alt="Assembling our robot">
      			    </div>
      			  </div>
              <p class="mb-5 text-left">To begin, we attached the servos to the mounts, and then attached them to the base of the robot. We then attached the wheels, which took some time because not all wheels fit nicely into the servos. We had to get a bit creative when attaching the ball bearing because there was only one left and it was too short for our robot. We ended up attaching a piece to the base of the robot to elongate the part where the ball bearing attached. This temporarily solved the issue. Lastly, we mounted the arduino and the battery with velcro.</p>
      			  <hr>
      			  <div class="media mb-3">
      			  	<div class="media-left">
      		       	<div class="embed-responsive video">
      				      <iframe width="560" height="315" src="https://www.youtube.com/embed/JGP3CFYPiuo?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      		 	  	  </div>
      			    </div>
      			    <div class="media-body">
      			      <h4 class="media-heading">Driving our robot autonomously</h4>
      			      <span>After assembling the robot, we wrote code for it to move autonomously. Initially, it was meant to move in a square, but the code did not work as expected and we did not have enough time to debug. The robot ended up moving in what appeared to be a pentagon.</span>
      			    </div>
      			  </div>
              <pre class="mb-0 text-left">
                <code>
void setup() {
  // we set up two of our pins as outputs and attach our servos
    int PWM1 = 3;
    int PWM2 = 5;
    pinMode(PWM1, OUTPUT);
    pinMode(PWM2, OUTPUT);
    Motor1.attach(PWM1);
    Motor2.attach(PWM2);
}

void loop() {
  // we write a loop to control the speed and direction that the motors spin
    Motor1.write(180);
    Motor2.write(0);
    delay(1200);
    Motor1.write(90);
    Motor2.write(90);
    delay(600);
    turn();
}

void turn(){
  // this function turns the robot
    Motor1.write(180);
    Motor2.write(180);
    delay(350);
    Motor1.write(90);
    Motor2.write(90);
    delay(250);
    return;
}
                </code>
              </pre>
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Lab1">View Lab 1 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Lab 1</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 2 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-2">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Lab 2: Analog Circuitry and FFTs</h2>
              <hr class="star-dark mb-5">
              <h3 class="text-secondary mb-1">Acoustic Team</h3>
              <h4 class="text-secondary mb-3">Liliet Sosa and Glenna Zhang</h4>
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-5 text-left">
                Create a microphone circuit that will detect a 660 Hz whistle blow signifying the beginning of our maze mapping.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Prelab</h4>
              <p class="mb-3 text-left">
                We need at least a 660Hz x 2 = 1320Hz sampling rate based on the Nyquist sampling rate to avoid aliasing.<br />
                Using the sketch found <a href="https://playground.arduino.cc/Main/ShowInfo">here</a>, these are the results of our speed test:
              </p>
              <img class="img-fluid mb-5" src="img/Lab 2 Photos/ShowInfoResult.png" alt="Serial Monitor result of ShowInfo"></img>
              <p class="mb-3 text-left">
                Based on this, <code>analogRead()</code> is sufficient (don’t need to use ADC) because its maximum rate is 111.987&#181s / sample = 1/111.987e-6 samples/s = 8930Hz, which is more than enough to detect the 660 Hz frequency.<br />
                We used the following materials:
              </p>
              <ul class="mb-5 text-left">
                <li><a href="https://www.mouser.com/datasheet/2/670/cma-4544pf-w-1309465.pdf">Electret microphone</a></li>
                <li>One 6.8&#181F polarized capacitor</li>
                <li>Resistors: 330&#8486, 3.8k&#8486, 380k&#8486, 10k&#8486 x2</li>
                <li>LM358 Op-amp</li>
                <li>Wires</li>
              </ul>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Unit Tests</h4>
              <p class="mb-3 text-left">
                <i>Unit Test 1: Set up a signal generator to deliver a signal that matches what you expect to see from your sensor. This signal must be between 0-5V to not damage the Arduino. Test that the frequency output from your signal generator matches what you see on the serial monitor.
                </i><br />
                <br />
                The following is an image of our signal generator (set to 660Hz, 2V<sub>pp</sub>, 1V offset)
              </p>
              <img class="img-fluid mb-3" src="img/Lab 2 Photos/SigGen.jpg" alt="photo of signal generator"></img>
              <p class="mb-3 text-left">
                We then coded a quick FFT analysis and output the results to the serial monitor:
              </p>
              <img class="img-fluid mb-3" src="img/Lab 2 Photos/SerialOutput.png" alt="photo of signal generator"></img>
              <p class="mb-0 text-left">
                For each line in the image above, the first number is the output of the FFT, and the second number is the index of the bin for readability. As you can see, we have a peak at 0 and at 19. Our bin size should be around 8930Hz / 256samples  = 34.883Hz/bin
                So, our results make sense but are off by seemingly 1 bin.<br />
                <br />
                Our FFT code:
              </p>
              <pre class="mb-0 text-left"><code>
#define LOG_OUT 1 // use the log output function
#define FFT_N 256 // set to 256 point fft

#include &ltFFT.h&gt // include the library

void setup() {
  Serial.begin(9600); // use the serial port
}

void loop() {
  cli();  // UDRE interrupt slows this way down on arduino1.0
  for (int i = 0 ; i < 512 ; i += 2) { // save 256 samples
    fft_input[i] = analogRead(A4); // put real data into even bins
    fft_input[i+1] = 0; // set odd bins to 0
  }
  fft_window(); // window the data for better frequency response
  fft_reorder(); // reorder the data before doing the fft
  fft_run(); // process the data in the fft
  fft_mag_log(); // take the output of the fft
  sei();
  Serial.println("start");
  String out = "";
  for (byte i = 0 ; i < FFT_N/2 ; i++) { 
    Serial.println(out + fft_log_out[i] + " " + i); //send out data
  }
  while(1) {} // we inserted this so that it only prints one result
}
              </code></pre>
              <p class="mb-3 text-left">
                <i>Unit Test 2: Use the app you downloaded during the pre-lab to generate a 660Hz tone. Measure the output from the microphone with the oscilloscope, and try to get an idea of what you need to do to the signal to be able to detect it securely from the Arduino.
                </i><br />
                <br />
                The following is the microphone circuit thus far, as taken from the lab handout:
              </p>
              <img class="img-fluid mb-2" src="img/Lab 2 Photos/MicrophoneCircuit_OG.png" alt="Original microphone circuit"></img>
              <p class="mb-3 text-left">
                After generating the tone, the microphone could not pick up the signal, but we knew the microphone was working because if we blew into the microphone the output would obviously change, as shown below:
              </p>
              <p class="mb-4"><b>Resting state of microphone</b></p>
              <img class="img-fluid mb-4" src="img/Lab 2 Photos/Microphone_Resting.jpg" alt="Output of microphone resting"></img>
              <p class="mb-4"><b>Microphone being blown on</b></p>
              <img class="img-fluid mb-4" src="img/Lab 2 Photos/Microphone_Blowing.jpg" alt="Output of microphone resting"></img>
              <p class="mb-3 text-left">
                From this result, we decided to add an amplifier and a filter. After struggling to get our own design to work, we decided to use <a href="https://cei-lab.github.io/ECE3400-2017-teamAlpha/lab2.html">Team Alpha's design</a> that incorporates an inverting amplifier, has a voltage divider to center the output voltage around 2.5V, and has a gain of 100:
              </p>
              <img class="img-fluid mb-2" src="img/Lab 2 Photos/MicCircuit.png" alt="Output of microphone resting"></img>
              <p class="mb-3 text-left">
                <i>Unit Test: Check your circuitry before hooking it up to the Arduino.</i><br />
                <br />
                Below is the result of using the amplifier while playing a 660Hz tone, displayed on the oscilloscope:
              </p>
              <img class="img-fluid mb-3" src="img/Lab 2 Photos/Microphone_Signal.jpg" alt="Output of microphone resting"></img>
              <p class="mb-3 text-left">
                Clearly, this signal can be sent into the Arduino as its minimum voltage is 2.12V and its peak-to-peak voltage is 840mV, which is well within the 0-5V range. The frequency is also correct.<br />
                The following is a picture of the physical circuit, where the purple wire is the output signal:
              </p>
              <img class="img-fluid mb-5" src="img/Lab 2 Photos/Circuit.jpg" alt="Output of microphone resting"></img>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Results</h4>
              <p class="mb-3 text-left">
                After connecting the circuit to our robot and changing the code that controls how the robot starts to incorporate waiting for the 660Hz signal, this is a video of our successful result:
              </p>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/aNj-LKDwgoE?rel=0" frameborder="0" allow="autoplay; encrypted-media" class="mb-3" allowfullscreen></iframe>
              <p class="mb-0 text-left">
                The following is our updated line following code to show that our robot waits for a signal before starting:
              </p>
              <pre class="mb-0 text-left"><code>
// In set-up, we wait while the signal has not been heard and the button has not been pressed:
while(!readSignal() && digitalRead(8) != HIGH);

// The following is our readSignal() method that calculates the FFT and returns true or false based on if the signal has been heard. We are reading the analog signal from A4:
boolean readSignal() {
  cli();  // UDRE interrupt slows this way down on arduino1.0
  for (int i = 0 ; i < 512 ; i += 2) { // save 256 samples
    fft_input[i] = analogRead(A4); // put real data into even bins
    fft_input[i+1] = 0; // set odd bins to 0
  }
  fft_window(); // window the data for better frequency response
  fft_reorder(); // reorder the data before doing the fft
  fft_run(); // process the data in the fft
  fft_mag_log(); // take the output of the fft
  sei();
  if (fft_log_out[19] >= 50){ // check that bin 19 contains a significant value
    return true;
  }
  return false;
}
              </code></pre>
              <hr class="mb-5">
              <h3 class="text-secondary mb-1">Optical Team</h3>
              <h4 class="text-secondary mb-3">Anthony Viego and Kevin Ying</h4>
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-5 text-left">
                Create a circuit which is able to detect a 6.08kHz IR signal signifying other robots which have to be avoided in the maze while discarding 18kHz decoy IR signals.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Prelab</h4>
              <p class="mb-3 text-left">
                Unlike the microphone circuit, using AnalogRead will not be fast enough for IR purposes. 
                We want to be able to detect 6kHz signals, which would require a sampling frequency of 
                approximately 12kHz. However, the maximum with AnalogRead is approximately 8930Hz, 
                as discussed above. Instead, we will have to read directly from the ADC pin result 
                register to increase our sampling frequency. Since we will have to sample at such a high frequency, 
                it will be difficult to do other processing simultaneously - we will have to carefully 
                determine when to measure as to not miss other robots while were are making decisions 
                about where to drive.
              </p>
              <p class="mb-3 text-left">
                Thankfully, we don't have to worry as much about other sources of IR, which are unlikely to occur at the same frequency. For example, one common source of IR is fluorescent lights, but they emit approximately 30-60kHz signals. Testing in lab confirms that even in the presence of artificial lights and sunlike, the main source of noise for IR is 60Hz coming from the walls.
              </p>

              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Frequency Detection and Amplification Circuit</h4>
              <p class="mb-3 text-left">
                In order to build the amplification circuit, we needed the following materials in addition to our usual Arduino Uno setup:
              </p>
              <ul class="mb-5 text-left">
                <li class="li-center">2x OP598A Phototransistor</li>
                <li class="li-center">2x LM358 Dual OP-Amp</li>
                <li class="li-center">1x 430 Ohm Resistor</li>
                <li class="li-center">1x Solderless breadboard</li>
                <li class="li-center">2x 1k Resistor</li>
                <li class="li-center">1x 10k Resistor</li>
                <li class="li-center">1x 100k Resistor</li>
                <li class="li-center">1x 24k Resistor</li>
                <li class="li-center">1x 10nF Capacitor</li>
              </ul>
              <p class="mb-3 text-left">
                Overall our circuit for detecting other robots can be broken down into 3 stages. 
                A picture of our completed IR circuit is below. You can see that one the top right
                of the image, there exist two phototransistors in series. That then feeds into the
                op-amp ICs, and the output commes out of the light blue wire which goes off-screen
                to the left.
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/IR_circuit.jpg" alt="Completed IR Capture and Amplification Circuit"></img>
                <figcaption>Figure: Completed IR Capture and Amplification Circuit</figcaption>
              </figure>
              
              <h5 class="text-secondary text-left mb-3">Stage 1: IR Capture</h4>
              <p class="mb-3 text-left">
                The first stage of our circuit is similar to the suggested circuit in the <a href="https://cei-lab.github.io/ece3400-2018/lab2.html">lab 2 handout</a>. However, rather we chose to use two resistors in parallel and two phototransistors in series to increase the amplitude of the produced signal. Figure 1 shows this stage of the circuit. As the output of our circuit ranged from 20mVpp to 100mVpp at very close distances, we decided to amplify the output of this stage to increase the distance we can detect other robots from. 
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/IR_capture_circuit.png" alt="IR Capture Circuit"></img>
                <figcaption>Figure: IR Capture Circuit</figcaption>
              </figure>
              <p class="mb-3 text-left">
                Once we implemented the output capture circuit, we probed the output of 
                the first stage with an oscilloscope when holding a treasure close to 
                the phototransistors. The treasures were tunable in frequency, so we 
                were able to output both a 6.2kHz signal and a 18kHz signal, which 
                approximate the frequencies emitted by the other robots and the decoy 
                robots in the maze respectively. While there is a visible signal, its 
                peak-to-peak voltage is very small, meaning that the Arduino would not 
                easily be able to tell when the signal was present. In addition, in 
                order to even get this clear of a signal, we needed to hold the IR 
                source very close to the phototransistors, which would not be a viable 
                solution for detecting robots from several inches away. We needed to 
                add analog amplification.
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/scope_no_amp.png" alt="Oscilloscope Output of IR Signal Before Amplification"></img>
                <figcaption>Figure: Output of IR Signal Before Amplification</figcaption>
              </figure>

              <h5 class="text-secondary text-left mb-3">Stage 2: Amplification</h4>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/ir_amp_circuit.png" alt="Amplification Circuit Diagram"></img>
                <figcaption>Figure: Amplification Circuit Diagram</figcaption>
              </figure>
              <p class="mb-3 text-left">
                This stage is responsible for amplifying the output of our IR capture circuit. 
                The circuit itself can be seen in figure 2 below. It should be noted that the 
                output of the IR capture circuit has a DC bias of approximately 2.5V. Since we 
                plan to have 20x gain, this would mean our signal would always hit the max voltage 
                of 5V and would appear to be a constant high signal. Thus, to remove this bias 
                we implemented a high pass RC filter using a 10k resistor and 10nF capacitor 
                which has a cutoff frequency of roughly 1592Hz. 
              </p>
              <p class="mb-3 text-left">
                We then fed the output of the low pass filter into a non-inverting amplifier. 
                We used a 20k resistor and 1k resistor set up in a negative feedback loop to 
                achieve a gain of roughly 20x. We found that this amplification level gave us 
                the clearest results on our output.
              </p>
              
              <h5 class="text-secondary text-left mb-3">Stage 3: Comparator</h4>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/ir_comp_circuit.png" alt="Comparator Circuit Diagram"></img>
                <figcaption>Figure: Comparator Circuit Diagram</figcaption>
              </figure>
              <p class="mb-3 text-left">
                In order to further amplify our output, we fed the output of the amplification 
                circuit to a comparator circuit, which we hoped would further increase the 
                gain of our circuit and improve the signal quality. We first sent the input 
                through a unity gain voltage buffer which ensured that as we experimented 
                with different types of amplifiers and comparator circuits, the gain of each 
                would act independently. With our final design, we no longer need the unity 
                gain amplifier and may remove it to compact our circuit. 
              </p>
              <p class="mb-3 text-left">
                The second half uses a comparator with a voltage generated with a voltage divider. 
                We first determined a voltage that we wanted to compare against by stepping 
                through voltage levels on a DC power supply until the desired result was achieved. 
                We found that we were able to get the clearest signal from the greatest range of 
                distances for the IR treasure for a comparator reference voltage of 
                approximately 23mV. The values of the resistors in the voltage divider were then 
                determined to approximate that voltage, since 5V * (430/100000) = 21.5mV.
              </p>
              <p class="mb-3 text-left">
                With the new amplifier and comparator, we were able to achieve the following output even when the IR emitter treasure was held 3-4 inches away from the phototransistors.
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/ir_with_amp.png" alt="Oscilloscope Output of IR Signal With Amplification"></img>
                <figcaption>Figure: Output of IR Signal With Amplification</figcaption>
              </figure>

              <h4 class="text-secondary text-left mb-3">FFT</h4>
              <p class="mb-3 text-left">
                Once we had a high signal-to-noise ratio and a clear signal, we now wanted to 
                be able to read data into the Arduino and process the inputs to determine the 
                frequencies coming in. To do this we connected the output of our comparator 
                circuit to pin A0 of the arduino. We then used adc0 to read in the output of 
                the comparator circuit from the ADCL and ADCH registers and converted these 
                values into a single 16 bit integer. These values were then stored in the 
                <code>fft_input</code> array which was then used by the fft library to calculate the 
                magnitude of each bin. The graph below shows the result of this operation 
                for a 6kHz and 18kHz signal. 
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 2 Photos/ir_6k_18k_comparison.png" alt="Overlaid FFT Output for Amplified IR Sensor Signal"></img>
                <figcaption>Figure: Overlaid FFT Output for Amplified IR Sensor Signal</figcaption>
              </figure>
              <p class="mb-3 text-left">
                As can be seen, the peak magnitude of the 6kHz signal is around bin 40. This 
                result is relatively accurate as our base clock for the arduino is 16MHz with 
                an ADC clock prescaler of 32. As each ADC read takes 13 clock cycles this 
                results in a sampling frequency of 38461.53. As we take 256 samples this 
                means that each bin represents 150.24 Hz. If we divide 6.08kHz by 150.24Hz 
                we find that the peak of our 6kHz single should be in bin 40 which is what 
                we found. 
              </p>
              <p class="mb-3 text-left">
                We then use this information to determine when a 6kHz signal is present by 
                checking to see if the magnitude of bins 38 - 44 are above 150. We decided 
                to use 150 as our threshold since general noise did not seem to create peaks 
                above 75 and the 18kHz signal also does not generate peaks in that range 
                above 100. By checking a range of bins we are able to determine if an 
                approximate 6kHz signal is present while having some tolerance incase the 
                frequency is not exact. The code, which is inside of a loop, is as follows:
              </p>
              <pre class="mb-0 text-left"><code>
cli();  // UDRE interrupt slows this way down on arduino1.0 so we disable interrupts

// save 256 samples
for (int i = 0 ; i < 512 ; i += 2) {
  while(!(ADCSRA & 0x10)); // wait for adc to be ready
  ADCSRA = 0xf5; // restart adc
  byte m = ADCL; // fetch adc data
  byte j = ADCH;
  int k = (j << 8) | m; // form into an int
  k -= 0x0200; // form into a signed int
  k <<= 6; // form into a 16b signed int
  fft_input[i] = k; // put real data into even bins
  fft_input[i+1] = 0; // set odd bins to 0
}

// process FFT data
fft_window(); // window the data for better frequency response
fft_reorder(); // reorder the data before doing the fft
fft_run(); // process the data in the fft
fft_mag_log(); // take the output of the fft

sei(); // re-enable interrupts
Serial.println("start");
for (byte i = 0 ; i < FFT_N/2 ; i++) { 
  Serial.println(fft_log_out[i]); // send out the data for off-chip processing
}
delay(1000);
for (int j = 38; j < 44; ++j) {
  if (fft_log_out[j] >= 150){
    digitalWrite(LED_BUILTIN, HIGH);   // turn the LED on (HIGH is the voltage level)
    delay(1000);                       // wait for a second
    digitalWrite(LED_BUILTIN, LOW);    // turn the LED off by making the voltage LOW
    delay(1000);                       // in reality, delay does not block other cmds so LEDs flash
  }
}
              </code></pre>
              <p class="mb-3 text-left">
                The results can be shown below.
              </p>
              <iframe class="mb-5" width="560" height="315" src="https://www.youtube.com/embed/GuVrt4HzFvU?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

              <hr class="mb-5">
              <h3 class="text-secondary mb-1">The Final Integration</h3>
              <p class="mb-3 text-left">
                We then combined the results of the IR team and the acoustic team into one larger project!
              </p>
              <iframe class="mb-5" width="560" height="315" src="https://www.youtube.com/embed/RVqd22eCrKk?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Lab2">View Lab 2 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 3 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-3">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Lab 3: System Integration and Radio Communication</h2>
              <hr class="star-dark mb-5">
              <h3 class="text-secondary mb-1">Radio Team</h3>
              <h4 class="text-secondary mb-3">Kevin Ying and Glenna Zhang</h4>
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-5 text-left">
                In this lab we established a communication protocol between one Arduino, which will be on our robot, and another, which will remain at a base station and need to interface with a GUI to display the progress of our robot. This interface will also come in handy for debugging because we will be able to determine the progress of our robot and what it has discovered.<br />
                <br />
                In order to communicate wirelessly, we used a pair of Nordic nRF24L01+ transceivers, as well as the relevant libraries. (can be found <a href="https://cei-lab.github.io/ece3400-2018/lab3.html">here</a>)
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Data Scheme & Protocol</h4>
              <p class="mb-3 text-left">
                We use the following two variables to store the current location of our robot:<br />
                <code>uint8_t x = 0; // stores the x index/coordinate</code><br />
                <code>uint8_t y = 0; // stores the y index/coordinate</code><br />
                We use the following 2-D array to store what we discover about each tile of the maze:<br />
                <code>uint8_t maze[9][9] = {...};</code>
                <br />
                We have also defined a set of macros that help us easily construct the correct bit sequence that describes what our robot sees, based on our protocol:
              </p>
              <pre class="mb-0 text-left">
                <code>
// walls 
#define bm_wall       15 << 0
#define bm_wall_east  1 << 1
#define bm_wall_north 1 << 3
#define bm_wall_west  1 << 0
#define bm_wall_south 1 << 2

// treasure
#define treasure_shift 4
#define bm_treasure_none     0 << 4
#define bm_treasure_b_sq     1 << 4
#define bm_treasure_r_sq     2 << 4
#define bm_treasure_b_ci     3 << 4
#define bm_treasure_r_ci     4 << 4
#define bm_treasure_b_tr     5 << 4
#define bm_treasure_r_tr     6 << 4

// whether square explored
#define bm_explored     1 << 7
#define bm_not_explored 0 << 7
#define explored_shift  7

// presence of other robot
#define bm_robot    1 << 1
#define bm_no_robot 0 << 1
#define robot_shift 1
                </code>
              </pre>
              <p class="mb-3 text-left">
                We learned that the maximum package the Nordic Radio module can send is 32 bytes, but we realized we only need to send 2 bytes of data to fully describe each tile, which means we were able to use a single transmission/packet per tile/cell. The following image describes the structure of our protocol:
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Lab 3 Photos/Protocol.png" alt="Protocol of our communication method"></img>
                <figcaption>Figure 1: Communication protocol</figcaption>
              </figure>
              <p class="mb-5 text-left">
                We believe that this protocol provides sufficient information about the state of the board. It includes the x and y coordinates of the current tile, which has just been explored. The explored bit is mainly for the robot itself to know if it has explored a tile. The treasure bits represent one of seven options: no treasure, blue square, red square, blue circle, red circle, blue triangle, and red triangle. Lastly, the for the bits representing each wall (north, south, east, west), a 1 represents that the wall is present in this tile and a 0 means it isn’t there.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Sending maze information wirelessly between Arduinos</h4>
              <p class="mb-3 text-left">
                We wrote a program to make a virtual robot explore a 9x9 maze with preset walls and treasures in order to test our radio communication and our protocol. We hard-coded the robot’s sequence of exploration for ease- it just snakes around the maze, as there is only one path anyway. We set the role of the virtual robot to “role_ping_out”, a.k.a. the transmitter, and we set the other Arduino to “role_pong_back”, or the receiver.<br />
                <br />
                The following is our setup code, which is very similar to the setup found in the GettingStarted sketch in the RF24 Arduino library:
              </p>
              <pre class="mb-0 text-left">
                <code>
void setup(void) {
  // Print preamble
  Serial.begin(9600);
  printf_begin();
  printf("\n\rRF24/examples/GettingStarted/\n\r");
  printf("ROLE: %s\n\r",role_friendly_name[role]);
  printf("*** PRESS 'T' to begin transmitting to the other node\n\r");

  // Setup and configure rf radio
  radio.begin();
  radio.setRetries(15,15);
  radio.setAutoAck(true);
  radio.setChannel(0x50);
  // set the power
  radio.setPALevel(RF24_PA_HIGH);
  radio.setDataRate(RF24_250KBPS);

  // optionally, reduce the payload size.  seems to
  // improve reliability
  radio.setPayloadSize(2); // we only need 2 bytes

  // Open pipes to other nodes for communication
  // This simple sketch opens two pipes for these two nodes to communicate
  // back and forth.
  // Open 'our' pipe for writing
  // Open the 'other' pipe for reading, in position #1 (we can have up to 5 pipes open for reading)
  radio.openWritingPipe(pipes[0]);
  radio.openReadingPipe(1,pipes[1]);
  
  // Start listening
  radio.startListening();

  // Dump the configuration of the rf unit for debugging
  radio.printDetails();
}
                </code>
              </pre>
              <p class="mb-0 text-left">
                Then, in the loop, we first develop the message that we are trying to send using the current cell information and the current location. We also print the message in binary for debugging:
              </p>
              <pre class="mb-0 text-left">
                <code>
uint8_t cell = maze[x][y]; // information about the current location, which our robot will use its sensors to determine
uint16_t coordinate = x << 4 | y; // generate the left byte of information (coordinates)
uint16_t message = coordinate << 8 | cell; // generate the whole message
Serial.println(message, BIN);
                </code>
              </pre>
              <p class="mb-0 text-left">
                The rest of the communication code is the same as in the GettingStarted sketch, but we changed what happens when the response is received- if a good response is received, we then let the robot move to the next tile, but if no response is received (timeout), then the robot stays put and tries again in the next iteration. This way no information is lost. The code is below:
              </p>
              <pre class="mb-0 text-left"><code>
// Describe the results
if ( timeout )
{
  printf("Failed, response timed out.\n\r");
}
else
{
  // Grab the response, compare, and send to debugging spew
  unsigned long got_time;
  radio.read( &got_time, sizeof(unsigned long) );

  // Spew it
  printf("Got response %lu, round-trip delay: %lu\n\r",got_time,millis()-got_time);
    
  if (x == 8 && y == 8) {
      x = 0;
      y = 0;
  }
  else if (x%2 == 0){
      if (y == 8) x++;
      else y++;
  }
  else{
      if (y == 0) x++;
      else y--;
  }
}

 // Try again 1s later
delay(1000);
              </code></pre>
              <p class="mb-0 text-left">
                We wrote another program to be the receiver of the messages and to update the GUI. The setup is largely the same as the transmitter program, so we will only show the looping code. It receives a packet and then proceeds to decode it using bitwise operators and our macros, and then sends the information in the correct format to the GUI (location, walls, and treasures, then a new line). Lastly, it resumes listening for a new packet.
              </p>
              <pre class="mb-0 text-left"><code>
// if there is data ready
if ( radio.available() )
{
  // Dump the payloads until we've gotten everything
  uint16_t message;
  unsigned char coord;
  unsigned char walls;
  bool done = false;
  while (!done)
  {
    // Fetch the payload, and see if this was the last one.
    // spew out payloads
    done = radio.read( &message, sizeof(uint16_t) );

    int x = (message & (15 << xcoord_shift)) >> xcoord_shift;
    int y = (message & (15 << ycoord_shift)) >> ycoord_shift;
    bool north = message & bm_wall_north;
    bool south = message & bm_wall_south;
    bool east = message & bm_wall_east;
    bool west = message & bm_wall_west;
    uint8_t treasure = (message & bm_treasure);
    char buff[30];
    sprintf(buff, "%d,%d", x, y);
    Serial.print(buff);

    /*
     * Code for checking the message with each wall/treasure bitmask goes here
     * If a match is found, more is printed to Serial
     */

    // Ends printed message to GUI
    Serial.print("\n");

    // Delay just a little bit to let the other unit
    // make the transition to receiver
    delay(20);

  }

  // First, stop listening so we can talk
  radio.stopListening();

  // Send the final one back.
  radio.write( &coord, sizeof(unsigned long) );
  //printf("Sent response.\n\r");

  // Now, resume listening so we catch the next packets.
  radio.startListening();
}
              </code></pre>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Updating the GUI</h4>
              <p class="mb-3 text-left">
                This is the final result of using our virtual robot to wirelessly update the GUI!
              </p>
              <iframe class="mb-5" width="560" height="315" src="https://www.youtube.com/embed/oeo-r_0Xa80?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <hr class="mb-5">
              <h3 class="text-secondary mb-1">Robot Team</h3>
              <h4 class="text-secondary mb-3">Liliet Sosa and Anthony Viego</h4>
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-3 text-left">
                The goal for this part of the lab was to integrate all of our robot’s sensing components, including recognizing the start signal of 660Hz with a microphone, line following, right-hand wall following, and detecting other robots while ignoring decoys. All of these features have been successfully built in previous labs/milestones, and so now we’ve integrated all of these features to prove that they can work together without interfering with each other.
              </p>
              <p class="mb-3 text-left">
                Integrating all of our separate circuits and systems in order to complete lab 3 turned out to be more of a challenge than initially anticipated. For starters, our IR detection circuit, microphone circuit, sensor inputs, and servo controls all existed either on different breadboards or very far away from the arduino. This created a large mess of wires that made modifying and debugging our circuits difficult as every modification almost always resulted in accidently removing another wire. 
              </p>
              <p class="mb-3 text-left">
                Cleaning up, we moved all of our circuitry to two half-breadboards which we mounted on the top level of our robot, with our arduino now being mounted on the lower level. This gave us better access to all of our circuitry as well as it helped to reduce the nest of wires making our robot easier to debug and cleaner looking as well. However, as we re-wired out inputs and outputs from our arduino we ran into the problem that we had more analog inputs than the arduino could handle. 
              </p>
              <p class="mb-3 text-left">
                To get around this problem, we used the CD74HC4051 analog multiplexer to switch between our wall sensors, since we only needed to be able to read from those sensors fairly infrequently. As we currently only needed to mux between two inputs we grounded the S1 and S2 selection inputs and connected digital pin 7 of our arduino to S0 of the mux. We then connected the output channel of the mux to analog input pin A5. 
              </p>
              <p class="mb-3 text-left">
                A rather unanticipated issue that arose from using the mux was that both of our sensors appeared to be changing based off of the values from the right one. At first we believed that one of the sensors was simply broken since the mux had been functioning originally. However, we discovered that the mux does not sit properly in the breadboard, causing some of the pins to not be fully connected. This resulted in only one of the wall sensors ever being read. To fix this problem we needed to bend several of the pins in order to better fit the breadboard.
              </p>
              <p class="mb-3 text-left">
                Another unanticipated problem we faced was the noise on our 5V rails caused by the wall sensors. Due to this noise our IR and microphone circuits ceased to function correctly. Our initial solution to this problem was to add as many decoupling capacitors as possible but we found that this simply didn’t work well enough. Instead, we decided to use the provided 5-to-3.3 voltage regulator, in order to have a separate power line dedicated to the sensitive IR circuit.
              </p>
              <p class="mb-5 text-left">
                Finally, in order to integrate the maze GUI code with the robot maneuvering, we needed to keep track of the current orientation of the robot (which would get updated whenever we turn left/right), as well as update the internal maze state whenever we reach an intersection, at which point we are able to detect walls. Note that since we only broadcast the maze state one time, before it has been explored, some of the walls do not appear. This will be fixed once add a third wall sensor on the left side of the robot, so that we will be able to detect all of the walls at the same time.
              </p>
              <hr class="mb-5">
              <h3 class="text-secondary mb-1">Final Integration</h3>
              <p class="mb-3 text-left">First, we incorporated radio communication into our overall program, as shown in the video below. As you can see in the video, the robot now pauses for a bit at every intersection. We do this because this is when the robot broadcasts to the base station, and if the broadcast is unsuccessful, it stays put and keeps trying. The coordinates received in the base station are correct at every intersection, based on the starting coordinate of the robot being set to (0, 0). We did not show the actual GUI updating in this video, because it was inconvenient at the time of recording and we've already shown above that it works.</p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/N0hEJD_Hcbo?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <p class="mb-5 text-left">Finally, the following video shows everything incorporated into our robot (line following, wall following, radio communication, GUI, starting on 660Hz, responding to other robots, and ignoring decoys):</p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/Ex6m2g_6p3g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <p class="mb-5 text-left">This last video shows the same robot (running the same code) detecting the IR hat from a farther distance. </p>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/sQ3DKX8HY3o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <hr class="mb-5">
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Lab3">View Lab 3 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 4 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-4">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Lab 4</h2>
              <hr class="star-dark mb-5">
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/lfoqNJq3ZcA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/51e3VvY_uiE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <iframe class="mb-3" width="560" height="315" src="https://www.youtube.com/embed/bFCsnk18t3w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <p class="mb-3">Hey! Thanks for checking out our work. Unfortunately, we're still working on this part of our robot, but we'll add documentation for this milestone as soon as we finish. Please check back in a few weeks!</p>
              <p class="mb-5">In the meantime, you can take a look at the assignment that we are trying to complete <a href="https://cei-lab.github.io/ece3400-2018/lab4.html">here</a>.</p>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- MILESTONES ------------------------------------------------------------------------------>
    <!-- Portfolio Modal 5 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-5">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
             <h2 class="text-secondary text-uppercase mb-0">Milestone 1</h2>
              <hr class="star-dark mb-5">
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-3 text-left">
                The goal of Milestone 1 was to add line tracking and maze traversing functionality to the robot in the absence of walls and treasures. The end goal was to have a robot capable of following a figure eight pattern on the ground. We added the following additional materials:
              </p>
              <ul class="mb-3 text-left">
                <li>3 QRE1113 Sensors on SparkFun breakout boards (https://www.sparkfun.com/products/9453)</li>
                <li>A breadboard</li>
                <li>A new 3D printed base for our robot</li>
                <li>3D printed connector parts</li>
                <li>White (or black) electrical tape</li>
                <li>More wires</li>
              </ul>
              <img class="img-fluid mb-5" src="img/Robot2.jpg" alt="Our revamped robot">
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Line Detection</h4>
              <p class="mb-5 text-left">The robot needed to be able to navigate lines made of the electrical tape on a flat surface arranged in a grid pattern. In order to be able to detect where lines were on the ground, we used QRE1113 analog sensors. These were connected to an analog input pin on the Arduino which gave a value based on the reflectivity of nearby surfaces using IR, and were powered from the same 5V and GND powering the Arduino.<br />
              <br />
              We then moved the line sensor above the ground surface at various heights and above both the taped line and the adjacent surface in order to discover some baseline values for what reflectivities mean what surface. We used those baseline values to determine a threshold, below which meant that the line sensor was on a line, and above which the opposite was true. We could now tell whether or not a line sensor on our robot was above a line!
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Line Following</h4>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/DCLGwZAPSa0?rel=0" frameborder="0" allow="autoplay; encrypted-media" class="mb-3" allowfullscreen></iframe>
              <p class="mb-3 text-left">We used three sensors (left, center, and right) to figure out where the robot was at any point in time. These sensors were placed at the front of the robot, spaced so that the distance between the left and right sensors is approximately the width of the line. The robot is considered centered when only the center sensor is on the line. When the left and center sensors are on the line, then the robot is veering slightly right, and we must turn slightly left. Similarly, when the right and center sensors are on the line, then the robot is veering slightly left and we must turn slightly right. If only the left sensor is on the line, then the robot is veering too much to the right and we must turn left more quickly. Similarly, if only the right sensor is on the line, the robot is veering too much to the left and we must turn right more quickly.<br />
              <br />
              To detect an intersection, the analog value of each sensor is read using the analogRead function and then each value is compared to a light threshold. For example, when both the left AND right sensors read below the threshold value, meaning that both are directly above lines, we have determined that the robot is at an intersection and continue straight through the intersection in order to follow the line.</p>
              <pre class="mb-0 text-left">
                <code>
void loop() {
    // Our bot repeatedly moves forward then corrects itself
    forward();
    linefollow();
}

void forward(){
    // Moves the bot forward
    MotorLeft.write(83);
    MotorRight.write(95);
    delay(100);
}

void linefollow(){
    //Below 950 is white tape
    //Above 950 is dark
    LightDataC = analogRead(LightCenter);
    LightDataL = analogRead(LightLeft);
    LightDataR = analogRead(LightRight);
    if (LightDataC <= 950 && LightDataL > 950 && LightDataR > 950){
          // centered
          return;
    } else if (LightDataL <= 950 && LightDataR <= 950) {
          // intersection
          return;
    } else if (LightDataC <= 950 && LightDataL <= 950){
          // bot is veering right slightly, so we turn it left a bit
          MotorRight.write(92);
          MotorLeft.write(80);
          delay(400);
          return;
    } else if (LightDataC <= 950 && LightDataR <= 950){
          // bot is veering left slightly, so we turn it right a bit
          MotorRight.write(100);
          MotorLeft.write(88);
          delay(400);
          return;
    } else if (LightDataL <= 950){
          // bot is veering right a lot, so we turn it left more
          MotorRight.write(92);
          MotorLeft.write(80);
          delay(400);
          return;
    } else if (LightDataR <= 950){
          // bot is veering left a lot, so we turn it right more
          MotorRight.write(100);
          MotorLeft.write(88);
          delay(400);
          return;
    } else {
         // this case should never be reached!! if it does then y i k e s
    }
}
                </code>
              </pre>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Figure 8</h4>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/iWSI64gUKew?rel=0" frameborder="0" allow="autoplay; encrypted-media" class="mb-3" allowfullscreen></iframe>
              <p class="mb-0 text-left">To accomplish a figure 8 we used our line following code to determine when we are at an intersection. Since all of our IR line sensors are in the front we then continued forward for an additional 350ms to ensure that we would begin turning when the robot is in the center of the intersection. Since we had to turn left and right in a complex pattern, we then referenced an array of integers using a global incrementing variable as the index. If the array was a 1 at that point then we would turn left, if it was 0 we would turn right. To accomplish the turn, we spun around for approximately 500ms and then finished the turn using a while loop which uses our line following code to detect when we have aligned with the line. Once the turn completed we incremented our global variable and continued forward until the next intersection.<br />
              <br />
              At the top of our code, we initialized the following globals:
              </p>
              <pre class="mb-0 text-left">
                <code>
int turn[8] = {1,0,0,0,0,1,1,1};
int i = 0;
                </code>
              </pre>
              <p class="mb-0 text-left">
                We added a push button on our robot so that it waits in setup() until we push the button:
              </p>
               <pre class="mb-0 text-left">
                <code>
// The following code is all in setup():
pinMode(8, INPUT);
...
while(digitalRead(8) !=  HIGH);
                </code>
              </pre>
              <p class="mb-0 text-left">
                Our main loop moves the robot forward then corrects itself if necessary. While it's correcting itself, if it is at an intersection, it turns. We use turnRight() and turnLeft() as helper methods to turn the robot.
              </p>
              <pre class="mb-0 text-left">
                <code>
void loop() {
    // put your main code here, to run repeatedly:
    forward();
    linefollow();
    delay(20);
}

void turnRight(){
    MotorLeft.write(80);
    MotorRight.write(80);
    delay(600);
    while(!(LightDataC <= 775 && LightDataL > 775 && LightDataR > 775)){
      LightDataC = analogRead(LightCenter);
      LightDataL = analogRead(LightLeft);
      LightDataR = analogRead(LightRight);
    }
    MotorLeft.write(90);
    MotorRight.write(90);
    delay(100);
    return;
}

void turnLeft(){
    MotorLeft.write(100);
    MotorRight.write(100);
    delay(600);
      while(!(LightDataC <= 775 && LightDataL > 775 && LightDataR > 775)){
      LightDataC = analogRead(LightCenter);
      LightDataL = analogRead(LightLeft);
      LightDataR = analogRead(LightRight);
    }
    MotorLeft.write(90);
    MotorRight.write(90);
    delay(100);
    return;
}
                </code>
              </pre>
              <p class="mb-0 text-left">
                The linefollow function follows the same pattern as before, but we now navigate at intersections as follows:
              </p>
              <pre class="mb-0 text-left">
                <code>
if (LightDataL <= 775 && LightDataR <= 775) {
  // Intersection
  digitalWrite(LED_BUILTIN, HIGH);
  forward();
  delay(350);
  if (turn[i] == 1){
      turnRight();
  }
  else turnLeft();
  if (i ==7){
    i = 0;
  }
  else {
        i = i + 1;
  }
  digitalWrite(LED_BUILTIN, LOW);
  return;
}
                </code>
              </pre>
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Milestone1">View Milestone 1 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Milestone 1</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 6 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-6">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Milestone 2</h2>
              <hr class="star-dark mb-5">
              <h4 class="text-secondary text-left mb-3">Goal</h4>
              <p class="mb-5 text-left">
                The goal of Milestone 2 was to add right-hand wall-following and robot-detecting functionality to the robot while maintaining previously built line-tracking capabilities. To detect walls, we used IR distance sensors, one on the front of the robot and one on the right. We also added additional LEDs (in series with 330ohm resistors) in order to signal what the robot is thinking as it progress through the maze.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Robot Redesign</h4>
              <p class="mb-5 text-left">
                Many parts of our circuitry from previous labs and milestones were scattered across multiple breadboards and had to be powered by external power sources. Therefore, in order to have a contained drivable robot we needed to compactify our circuits and transfer them onto the main robot body. No major changes were made to either the IR detection or microphone circuit, although many of the specific pins we read have changed. We will continue to compact our circuit and make the connections more permanent/reliable throughout the rest of the lab sections. Note that for this milestone, we disconnect the microphone circuit entirely in order to make use of as many analog input pins as possible. In the future, we intend to free up analog inputs by replacing their usage with muxing or digital triggering circuits.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Circling a set of walls through right-hand wall-following</h4>
              <p class="mb-5 text-left">
                  As shown in the video below, the red LED turns on when the robot senses a wall in front of it, and the green LED turns on when it senses a wall to the right. These LEDs only change at an intersection in the grid, because we only check for walls at these intersections. We determined a threshold value for the distance sensors by holding a wall a certain distance away from the robot so that we don't detect a wall too early or too late. Our right-hand wall-following logic is determined through a series of if-statements.
              </p>
              <p class="mb-5 text-left">
                Firstly, in order to follow the wall to the right, we always want to turn right if there is no wall. Otherwise, if there is a wall to the right and no wall in front, we follow the wall forwards. Finally, if there are walls both the front and right, then we try to find a direction which does not have a wall in front by turning left. Each turn rotates the vehicle 90 degrees, so that once we know that there is no longer a wall both to the front and to the right, then there should be no wall to the front and we can continue driving forward. This allows us to reliably follow walls regardless of the configuration.
              </p>
              <iframe class="mb-5" width="560" height="315" src="https://www.youtube.com/embed/R7MomRgZwi8?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <p class="mb-0 text-left">The following is commented code on how we implemented this:</p>
              <pre class="mb-0 text-left">
                <code>
// this is some code from our linefollow() method from Milestone 1 that we changed for this milestone
void linefollow(){
    //Below LIGHTTHRESHOLD is white tape
    //Above LIGHTTHRESHOLD is dark
    LightDataC = analogRead(LightCenter);
    LightDataL = analogRead(LightLeft);
    LightDataR = analogRead(LightRight);

    bool leftOnLine = LightDataL <= LIGHTTHRESHOLD;
    bool centerOnLine = LightDataC <= LIGHTTHRESHOLD;
    bool rightOnLine = LightDataR <= LIGHTTHRESHOLD;
     
    if (centerOnLine && !leftOnLine && !rightOnLine) {
        // centered
        Serial.println("Centered");
        return;
    } else if (leftOnLine && rightOnLine) {
        forward();
        delay(650); // this allows the robot to be centered on top of the intersection before turning
        wallfollow(); // new method!
        Serial.println("intersection");
        return;
    }
    ... // other cases
}

// our wall-following code
void wallfollow(){
  wallRight = analogRead(A5);
  wallFront = analogRead(A4);
  if (wallRight >= SOMETHRESHOLD) digitalWrite(rightWallLED, HIGH); else digitalWrite(rightWallLED, LOW);   // turn the right wall LED on
  if (wallFront >= SOMETHRESHOLD) digitalWrite(frontWallLED, HIGH); else digitalWrite(frontWallLED, LOW);   // turn the front wall LED on
  if (wallFront <= SOMETHRESHOLD && wallRight >= SOMETHRESHOLD) { // if greater than threshold there is a wall 
      // following the right wall: we can go straight
      return;
  }
  if (wallRight <= SOMETHRESHOLD){  // nothing on the right, so we can turn right 
      turnRight(); // turns right until the center line sensor hits a line
      return;
  }
  while (wallFront >= SOMETHRESHOLD && wallRight >= SOMETHRESHOLD){ // blocked on both front and right- keep turning until we aren't blocked
      turnLeft(); // turns left until the center line sensor hits a line
      wallRight = analogRead(A5);
      wallFront = analogRead(A4);
      // signal our findings with LED
      if (wallRight >= SOMETHRESHOLD) digitalWrite(rightWallLED, HIGH); else digitalWrite(rightWallLED, LOW);
      if (wallFront >= SOMETHRESHOLD) digitalWrite(frontWallLED, HIGH); else digitalWrite(frontWallLED, LOW);
  }
  return;
}
                </code>
              </pre>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Avoiding other robots</h4>
              <p class="mb-5 text-left">
                In order to be able to detect and avoid other robots, we needed to integrate the IR FFT detection with the other analog input reading that our Arduino does. However, we quickly found that the FFT detection code from Lab 2 changed some of the serial communication registers and timers, which results in servos behaving strangely or not moving at all. In order to get around this obstacle, we stored the state of the required registers and then restore them once the FFT is processed and we return: TIMSK0, ADCSRA, ADMUX, and DIDR0. 
              </p>
              <figure>
                <img class="img-fluid mb-3" src="img/Milestone 2 Photos/Noise_5V.jpg" alt="Noise on Arduino 5V"></img>
                <figcaption>Figure 2: Noise Out of 5V Pin</figcaption>
              </figure>
              <p class="mb-5 text-left">
                We also found in our testing that our robot was failing to reliably detect walls once powered off of the Arduino's 5V line, due to coupling with other components or the operations within the Arduino. This meant that a single reading from the IR FFT code was not sufficient to ascertain that a robot was present, but the sensing was also not reliable enough that we could count on every single reading returning that there was a robot present. Therefore, we needed to have some way of keeping track of how often we thought there was a robot present. We required that several readings detect the presence of a 6kHz IR source in quick succession by having count of the number of readings that increases when the FFT says there is a 6kHz peak, and otherwise slowly decays. Then, we only tried to avoid robots when the reading count was high enough. This essentially allows us to filter through the noise.
              </p>
              <hr class="mb-5">
              <h4 class="text-secondary text-left mb-3">Line-tracking & avoiding robots and walls</h4>
              <p class="mb-5 text-left">
                We then were able to reliably do line tracking, wall avoidance, and robot avoidance together.
              </p>
              <iframe width="560" height="315" src="https://www.youtube.com/embed/XvThMN8REoo?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <hr class="mb-5">
              <a class="btn btn-secondary btn-lg rounded-pill" href="https://github.com/ECE3400Team28/website/tree/master/Milestone2">View Milestone 2 Code on GitHub</a>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 7 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-7">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Milestone 3</h2>
              <hr class="star-dark mb-5">
              <img class="img-fluid mb-5" src="img/portfolio/submarine.png" alt="">
              <p class="mb-3">Hey! Thanks for checking out our work. Unfortunately, we're still working on this part of our robot, but we'll add documentation for this milestone as soon as we finish. Please check back in a few weeks!</p>
              <p class="mb-5">In the meantime, you can take a look at the assignment that we are trying to complete <a href="https://cei-lab.github.io/ece3400-2018/Grading/Milestone_score.html">here</a>.</p>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Portfolio Modal 8 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-8">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Milestone 4</h2>
              <hr class="star-dark mb-5">
              <img class="img-fluid mb-5" src="img/portfolio/submarine.png" alt="">
              <p class="mb-3">Hey! Thanks for checking out our work. Unfortunately, we're still working on this part of our robot, but we'll add documentation for this milestone as soon as we finish. Please check back in a few weeks!</p>
              <p class="mb-5">In the meantime, you can take a look at the assignment that we are trying to complete <a href="https://cei-lab.github.io/ece3400-2018/Grading/Milestone_score.html">here</a>.</p>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close Project</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Contract Modal -->
    <div class="portfolio-modal mfp-hide" id="contract-modal">
      <div class="portfolio-modal-dialog bg-white">
        <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
          <i class="fa fa-3x fa-times"></i>
        </a>
        <div class="container text-center">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h2 class="text-secondary text-uppercase mb-0">Team 28 Contract</h2>
              <hr class="star-dark mb-5">
              <div class="contract-content">
                <h3>Team Members</h3>
                <p class="mb-5">Glenna Zhang, Liliet Sosa, Anthony Viego, Kevin Ying</p>
                <hr>
                <h3>Team Procedures</h3>
                <ol class="mb-5">
                  <li><b>Day, time, and place for regular team meetings:</b><br />
                  UH142, Friday 11.15-12.05pm, weekly (We will allocate more time towards the end of the semester, as determined during weekly meetings).</li>
                  <li><b>Preferred method of communication (e.g., e-mail, cell phone, wired phone, Blackboard Discussion Board, face-to-face, in a certain class) in order to inform each other of team meetings, announcement, updates, reminders, problems:</b><br />
                  Messenger and team meetings. Extra team meeting times will be communicated through Google Calendar.</li>
                  <li><b>Decision-making policy (by consensus? by majority vote?):</b><br />
                  We will try to make major design decisions by consensus. Major policy disagreements should allow for a waiting time, preferably overnight, before a final decision is made. If the team is still evenly split between decisions, the team will ask for the opinion of an instructor, and if necessary the instructor will have final say.</li>
                  <li><b>Method for setting and following meeting agendas (Who will set each agenda? When? How will team members be notified/reminded? Who will be responsible for the team following the agenda during a team meeting? What will be done to keep the team on track during a meeting?):</b><br />
                  The current team leader will take charge of setting the agenda, notifying team members, and keeping the team on track. Agendas will be written in our team’s shared Google Drive folder.</li>
                  <li><b>Method of record keeping (Who will be responsible for recording & disseminating minutes? How & when will the minutes be disseminated? Where will all agendas & minutes be kept?):</b><br />
                  The current team leader will be in charge of taking minutes. The minutes will be in our team’s shared google drive.</li>
                </ol>
                <hr>
                <h3>Team Expectations</h3>
                <div class="mb-5">
                  <h5>Work Quality</h5>
                  <ol>
                    <li><b>Project standards (What is a realistic level of quality for team presentations, collaborative writing, individual research, preparation of drafts, peer reviews, etc.?):</b><br />
                    The team will review and agree the level of the work is acceptable.</li>
                    <li><b>Strategies to fulfill these standards:</b>
                      <ul>
                        <li>Communication - standards should be clear at the start</li>
                        <li>Working hard</li>
                        <li>Team work!!</li>
                      </ul>
                    </li>
                  </ol>
                  <h5>Team Participation</h5>
                  <ol>
                    <li><b>Strategies to ensure cooperation and equal distribution of tasks:</b>
                      <ul>
                        <li>Team will agree on the division of tasks</li>
                        <li>Do your assigned task</li>
                        <li>Communicate with team members when encountering difficulties, even when this means possibly having to reassign tasks</li>
                      </ul>
                    </li>
                    <li><b>Strategies for encouraging/including ideas from all team members (team maintenance):</b>
                      <ul>
                        <li>Make sure everyone’s voices are being heard</li>
                        <li>Welcoming team environment</li>
                        <li>Ask other team members often for their ideas</li>
                      </ul>
                    </li>
                  <li><b>Strategies for keeping on task (task maintenance):</b>
                    <ul>
                      <li>Team leader makes sure agenda is being followed during meetings</li>
                      <li>Check on team members’ progress at the start of each meeting</li>
                      <li>Sub-teams/whole team schedules blocks of time to work together on assigned tasks</li>
                    </ul>
                  </li>
                  <li><b>Preferences for leadership (informal, formal, individual, shared):</b><br />
                    We want to keep leadership informal throughout the whole semester to make sure that the team atmosphere is positive. Since we will be rotating who the team leader is, it will be an individual responsibility.
                  </li>
                  </ol>
                  <h5>Personal Accountability</h5>
                  <ol>
                    <li><b>Expected individual attendance, punctuality, and participation at all team meetings:</b>
                      <ul>
                        <li>Members are expected to attend all meetings, be punctual and participate unless a valid reason is presented.</li>
                        <li>Members are expected to notify the team when conflicts occur.</li>
                      </ul>
                    </li>
                    <li><b>Expected level of responsibility for fulfilling team assignments, timelines, and deadlines:</b>
                      <ul>
                        <li>Members are expected to complete their assignments on-time.</li>
                        <li>Members are expected to create their own timelines and request help from other members to ensure that the work is completed.</li>
                      </ul>
                    </li>
                    <li><b>Expected level of communication with other team members:</b>
                      <ul>
                        <li>Members are expected to be active participants in face-to-face meetings and in online communication.</li>
                      </ul>
                    </li>
                    <li><b>Expected level of commitment to team decisions and tasks:</b>
                      <ul>
                        <li>Members are expected to be committed to a team decision and should not ignore team decisions to do things “their way”</li>
                        <li>Members are expected to vocalize any doubts or opposing opinions in order for the team to be able to come to a happy consensus</li>
                      </ul>
                    </li>
                  </ol>
                  <h5>Consequences for Failing to Follow Procedures and Fulfill Expectations</h5>
                  <ol>
                    <li><b>Describe, as a group, you would handle infractions of any of the obligations of this team contract:</b>
                      <ul>
                        <li>First infractions will result in a verbal warning.</li>
                        <li>Further infractions will result in a team discussion of the behavior</li>
                      </ul>
                    </li>
                    <li><b>Describe what your team will do if the infractions continue:</b>
                      <ul>
                        <li>Infracting members will have to take meeting minutes in place of the team leader.</li>
                        <li>Continued infractions will result in buying the team boba.</li>
                      </ul>
                    </li>
                  </ol>
                </div>
                <hr>
                <h3>Team Leadership</h3>
                <p class="mb-5">
                  <b>Every person on the team will have to take the role as a leader. The role of the leader will be to organize meetings and make sure that everything is submitted in a timely manner. Please note here who will be responsible when:</b><br />
                  <br />
                  Weeks 1&2: Kevin Ying<br />
                  Weeks 3&4: Glenna Zhang<br />
                  Weeks 5&6: Liliet Sosa<br />
                  Weeks 7&8: Anthony Viego<br />
                  Weeks 9&10: Glenna Zhang<br />
                  Weeks 11&12: Liliet Sosa<br />
                  Weeks 13&14: Kevin Ying<br />
                  Weeks 15&16: Anthony Viego<br />
                </p>
                <hr>
                <div class="mb-5">
                  <b>
                    I participated in formulating the standards, roles, and procedures as stated in this contract.<br />
                    I understand that I am obligated to abide by these terms and conditions.<br />
                    I understand that if I do not abide by these terms and conditions, I will suffer the consequences as stated in this contract.<br />
                  </b>
                  <br />
                  <ol>
                    <li>Anthony Viego, 8/31/18</li>
                    <li>Kevin Ying, 8/31/18</li>
                    <li>Glenna Zhang, 8/31/18</li>
                    <li>Liliet Sosa, 8/31/18</li>
                  </ol>
                </div>
              </div>
              <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                <i class="fa fa-close"></i>
                Close
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
	<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
    <script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/freelancer.min.js"></script>
	
   

  </body>

</html>
